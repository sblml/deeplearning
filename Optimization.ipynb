{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy vs BCELoss in Pytorch  \n",
    "https://jaeyung1001.tistory.com/45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlgoTrade  \n",
    "Test Loss incrases, but Test Accuracy also increase\n",
    "\n",
    "https://discuss.pytorch.org/t/why-is-the-validation-loss-increasing/68683\n",
    "\n",
    "One possible reason for the increasing validation loss would be that the model increases its logits for the wrong classes.\n",
    "\n",
    "Thanks for the answer! so to put it simply, can it be said that due to the intense overfitting, the model is now classifying the correctly classified images less better than before (e.g assigning them a lesser softmax probability than before). This while keeping the accuracy relatively constant, increases the loss. Will that be the correct interpretation of what you are saying?\n",
    "\n",
    "This might be the case or the model is classifying the wrong classes with a higher confidence.\n",
    "\n",
    "i.e. corrects are selected with less confidence  \n",
    "but correct selections increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = en_vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "Note: when scoring the performance of a language translation model in particular, we have to tell the nn.CrossEntropyLoss function to ignore the indices where the target is simply padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F.softmax vs F.log_softmax  \n",
    "`y = F.softmax(x, dim=1)`  \n",
    "`y = torch.log(y)`  \n",
    "is identical to  \n",
    "`z = F.log_softmax(x, dim=1)`\n",
    "\n",
    "but log_softmax is more stable than softmax then log\n",
    "\n",
    "CrossEntropyLoss = log_softmax + NLLLoss  \n",
    "therefore CrossEntropyLoss receives raw dense output\n",
    "\n",
    "however, can accept log_softmax output due to the following:  \n",
    "log_softmax(log_softmax(x)) = log_softmax(x)  \n",
    "although errors do not occur, clearly not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout just before the end LogSoftmax, in NLPfromScratch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Training the Network in NLPfromScratch2  \n",
    "\"The magic of autograd allows you to simply sum these losses at each step and call backward at the end.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "scheduler.step() at the end of epoch\n",
    "or\n",
    "if total_accu is not None and total_accu > accu_val:\n",
    "    scheduler.step()\n",
    "else:\n",
    "    total_accu = accu_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, we use nn.utils.clip_grad_norm_ function to scale all the gradient together to prevent exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing Gradient  \n",
    "https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/  \n",
    "X: 2-dim coordinates, y: labels  \n",
    "\n",
    "MLP (total 2 layers) of one hidden layer with tanh activation and uniform initialization works,  \n",
    "but of five hidden layers does not work due to vanishing gradients.  \n",
    "MLP of five hidden layers with ReLU activation and He initialization works  \n",
    "of 15 hidden layers also works, but of 20 hidden layers fails, as shown  \n",
    "***\n",
    "Review Average Gradient Size During Training  \n",
    "It is hard to diagnose a vanishing gradient as a cause for bad performance. One possible signal is to review the average size of the gradient per layer per training epoch.\n",
    "\n",
    "We would expect layers closer to the output to have a larger average gradient than those layers closer to the input.\n",
    "\n",
    "LinePlots  x-axis: epoch, y-axis: gradient  \n",
    "DensityPlots  x-axis: gradient, y-axis: epoch, \n",
    "\n",
    "The plots of the average gradient per layer per training epoch show a different story as compared to the gradients for the deep model with tanh.\n",
    "We can see that the first hidden layer sees more gradients, more consistently with larger spread, perhaps 0.2 to 0.4, as opposed to 0.05 and 0.1 seen with tanh. We can also see that the middle hidden layers see large gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradients is a particular problem with recurrent neural networks as the update of the network involves unrolling the network for each input time step, in effect creating a very deep network that requires weight updates. A modest recurrent neural network may have 200-to-400 input time steps, resulting conceptually in a very deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://en.wikipedia.org/wiki/Vanishing_gradient_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhttps://captum.ai/\n",
    "https://discuss.pytorch.org/t/how-to-check-for-vanishing-exploding-gradients/9019/13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
