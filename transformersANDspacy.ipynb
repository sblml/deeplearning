{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical workflows  \n",
    "initiate a model with model_type and model_name, e.g.,  \n",
    "'distilbert', 'distilbert-base-uncased-distilled-squad' in https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack  \n",
    "'roberta', 'roberta-large' in https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing  \n",
    "respectivly\n",
    "\n",
    "check model_doc for the selected model, especially,  \n",
    "overview  \n",
    "tokenizer to use  \n",
    "class transformers.{model_type}Model, outputting raw hidden-states without any specific head on top  \n",
    "see forward method about inputs required\n",
    "\n",
    "EXAMPLES  \n",
    "for roberta, check https://huggingface.co/transformers/model_doc/roberta.html  \n",
    "Byte-level BPE, token_type_ids not required, instead separate segments with </s>  \n",
    "\n",
    "for BERT, https://huggingface.co/transformers/model_doc/bert.html  \n",
    "based on WordPiece, check forward\n",
    "\n",
    "general methods for classes are MAIN CLASSES tab on the left\n",
    "for example, tokenizers are in https://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "see \\__call__ method\n",
    "\n",
    "but, tokenizer.from_pretrained() are in\n",
    "https://huggingface.co/transformers/internal/tokenization_utils.html#pretrainedtokenizerbase\n",
    "\n",
    "More on  \n",
    "*https://huggingface.co/transformers/training.html  \n",
    "https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheetSheat\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "encoding = tokenizer(df_train.text[0:5].tolist(), df_train.selected_text[0:5].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.BertTokenizer for online and offline  \n",
    "tokenizres.BertWordPieceTokenizer for offline only\n",
    "\n",
    "transformers.BertTokenizer.from_pretrained('bert-base-uncased' or path)  \n",
    "many other pretrained models are listed in  \n",
    "https://huggingface.co/transformers/_modules/transformers/tokenization_bert.html\n",
    "\n",
    "tokenizers.BertWordPieceTokenizer(\n",
    "    f\"{BERT_PATH}/vocab.txt\", \n",
    "    lowercase=True\n",
    ")\n",
    "vocab.txt can be obtained by  \n",
    "https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained\n",
    "\n",
    "More on https://www.kaggle.com/debanga/huggingface-tokenizers-cheat-sheet\n",
    "\n",
    "Since tweet-sentiment-extraction notebooks cannot access to the Internet, most kernels used tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.roberta  \n",
    "https://huggingface.co/transformers/model_doc/roberta.html  \n",
    "\n",
    "RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n",
    "\n",
    "RoBERTa doesn’t have token_type_ids, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or </s>)\n",
    "\n",
    "Checking for the architecture  \n",
    "roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH)  \n",
    "roberta\n",
    "\n",
    "embedding  \n",
    "encoder comprised of 12 RobertaLayer  \n",
    "pooler of Dense with tanh activation\n",
    "\n",
    "INPUT for roberta  \n",
    "forward(self, input_ids, attention_mask, token_type_ids, ...)  \n",
    "input_ids:  \n",
    "Indices of input sequence tokens in the vocabulary.  \n",
    "attention_mask:  \n",
    "Mask to avoid performing attention on padding token indices.  \n",
    "https://huggingface.co/transformers/glossary.html#attention-mask  \n",
    "This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.  \n",
    "token_type_ids:  \n",
    "https://huggingface.co/transformers/glossary.html#token-type-ids  \n",
    "Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:  \n",
    "The first sequence, the “context” used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers  \n",
    "https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "naive tokenizations generate a too big vocabulary so subword tokenization is used instaed\n",
    "\n",
    "Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model \n",
    "only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.\n",
    "\n",
    "Three main types: BPE, WordPiece, SentencePiece\n",
    "\n",
    "BPE  \n",
    "first perform naive pre-tokenization  \n",
    "create a base vocabulary of all symbols and learns merge rules  \n",
    ": 'hug' -> 'h' 'u' 'g' -> 'h' 'ug'  \n",
    "does to until the desired size\n",
    "\n",
    "Byte-level BPE  \n",
    "for roberta\n",
    "\n",
    "WordPiece  \n",
    "WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra.  \n",
    "very similar to BPE  \n",
    "WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to make ensure it’s worth it.\n",
    "\n",
    "SentencePiece  \n",
    "All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words.  \n",
    "All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/debanga/huggingface-tokenizers-cheat-sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "tokernizers to tokenize, numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab = f\"{ROBERTA_PATH}/vocab.json\",\n",
    "    merges = f\"{ROBERTA_PATH}/merges.txt\",\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.encode(STR)\n",
    "tok.offstes : \n",
    "tok.ids : the dictionary index for the words in STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding1 = tokenizer(df_train.text[0:5].tolist(), df_train.selected_text[0:5].tolist(), return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers vs simpletransformers  \n",
    "simpletransformers is the package built on transformers for easy use  \n",
    "model_type, model_name arguments are identical  \n",
    "?the form required for train data is also identical  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual steps for using simpletransformrs  \n",
    "Preparing train data as json of the format\n",
    "https://simpletransformers.ai/docs/qa-data-formats/\n",
    "\n",
    "Initiate a model with model_type and model_name  \n",
    "model_type : bert, roberta, distilled, etc.  \n",
    "model_name: specifies the exact architecture and trained weights to use  \n",
    "https://simpletransformers.ai/docs/usage/#task-specific-models  \n",
    "https://simpletransformers.ai/docs/qa-model/\n",
    "\n",
    "model.train_model(PATH_TO_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOW to use SPACY  \n",
    "codes similar here  \n",
    "https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Training-Models\n",
    "\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "    \n",
    "https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-ensemble?scriptVersionId=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training data for entity recognition  \n",
    "(text, {'entities': [(start, end, class)]})  \n",
    "text[start:end] -> entities in text  \n",
    "\n",
    "Example  \n",
    "('I like London and Berlin.', { 'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The belows are from https://www.kaggle.com/sblroid/tweet-sentiment-extraction-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(sentiment):\n",
    "    '''\n",
    "    Returns Trainong data in the format needed to train spacy NER\n",
    "    '''\n",
    "    train_data = []\n",
    "    for index, row in df_train.iterrows():\n",
    "        if row.sentiment == sentiment:\n",
    "            selected_text = row.selected_text\n",
    "            text = row.text\n",
    "            start = text.find(selected_text)\n",
    "            end = start + len(selected_text)\n",
    "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
    "            # (text, selected_text)\n",
    "            # selected_text = text[start:end]\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_path(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        model_out_path = 'models/model_pos'\n",
    "    elif sentiment == 'negative':\n",
    "        model_out_path = 'models/model_neg'\n",
    "    else:\n",
    "        model_out_path = None\n",
    "    return model_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, output_dir, n_iter=20, model=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(output_dir)\n",
    "        print(f\"Loaded model '{model}'\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        print(\"Created blank 'en' model\")\n",
    "        \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    # as a result, a pipe in nlp and we got ner to add labels\n",
    "        \n",
    "    for _, annotations in train_data:  # annotations : Dict\n",
    "        for ent in annotations.get('entities'):  # 'entities': [[start, end, 'selected_text']]\n",
    "            ner.add_label(ent[2])  # ent[2]: 'selected_text'\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        else:\n",
    "            nlp.resume_training()\n",
    "            \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=compounding(4, 500, 1.001))\n",
    "            # minibatch : given train_data, returns a generator yielding batches of the given size\n",
    "            # compounding(4, 500, 1.001):start 4, next 4 * 1.001, end 500\n",
    "            # in this example, size=4 and size=compounding(4, 500, 1.001) generates the same result\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # texts: a tuple of text of size 4 \n",
    "                # annotations: a tuple of dict of size 4\n",
    "                nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
    "            print('Losses', losses)\n",
    "    save_model(output_dir, nlp, 'st_ner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
