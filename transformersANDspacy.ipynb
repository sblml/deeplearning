{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical workflows  \n",
    "initiate a model with model_type and model_name, e.g.,  \n",
    "'distilbert', 'distilbert-base-uncased-distilled-squad' in https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack  \n",
    "'roberta', 'roberta-large' in https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing  \n",
    "respectivly\n",
    "\n",
    "More on  \n",
    "https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.roberta  \n",
    "https://huggingface.co/transformers/model_doc/roberta.html  \n",
    "\n",
    "Checking for the architecture  \n",
    "roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH)  \n",
    "roberta\n",
    "\n",
    "embedding  \n",
    "encoder comprised of 12 RobertaLayer  \n",
    "pooler of Dense with tanh activation\n",
    "\n",
    "INPUT for roberta  \n",
    "forward(self, input_ids, attention_mask, token_type_ids, ...)  \n",
    "input_ids:  \n",
    "Indices of input sequence tokens in the vocabulary.  \n",
    "attention_mask:  \n",
    "Mask to avoid performing attention on padding token indices.  \n",
    "https://huggingface.co/transformers/glossary.html#attention-mask  \n",
    "This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.  \n",
    "token_type_ids:  \n",
    "https://huggingface.co/transformers/glossary.html#token-type-ids  \n",
    "Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:  \n",
    "The first sequence, the “context” used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "tokernizers to tokenize, numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab = f\"{ROBERTA_PATH}/vocab.json\",\n",
    "    merges = f\"{ROBERTA_PATH}/merges.txt\",\n",
    "    lowercase=True,\n",
    "    add_prefix_space=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.encode(STR)\n",
    "tok.offstes : \n",
    "tok.ids : the dictionary index for the words in STR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers vs simpletransformers  \n",
    "simpletransformers is the package built on transformers for easy use  \n",
    "model_type, model_name arguments are identical  \n",
    "?the form required for train data is also identical  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual steps for using simpletransformrs  \n",
    "Preparing train data as json of the format\n",
    "https://simpletransformers.ai/docs/qa-data-formats/\n",
    "\n",
    "Initiate a model with model_type and model_name  \n",
    "model_type : bert, roberta, distilled, etc.  \n",
    "model_name: specifies the exact architecture and trained weights to use  \n",
    "https://simpletransformers.ai/docs/usage/#task-specific-models  \n",
    "https://simpletransformers.ai/docs/qa-model/\n",
    "\n",
    "model.train_model(PATH_TO_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOW to use SPACY  \n",
    "codes similar here  \n",
    "https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Training-Models\n",
    "\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "    \n",
    "https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-ensemble?scriptVersionId=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training data for entity recognition  \n",
    "(text, {'entities': [(start, end, class)]})  \n",
    "text[start:end] -> entities in text  \n",
    "\n",
    "Example  \n",
    "('I like London and Berlin.', { 'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The belows are from https://www.kaggle.com/sblroid/tweet-sentiment-extraction-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(sentiment):\n",
    "    '''\n",
    "    Returns Trainong data in the format needed to train spacy NER\n",
    "    '''\n",
    "    train_data = []\n",
    "    for index, row in df_train.iterrows():\n",
    "        if row.sentiment == sentiment:\n",
    "            selected_text = row.selected_text\n",
    "            text = row.text\n",
    "            start = text.find(selected_text)\n",
    "            end = start + len(selected_text)\n",
    "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
    "            # (text, selected_text)\n",
    "            # selected_text = text[start:end]\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_path(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        model_out_path = 'models/model_pos'\n",
    "    elif sentiment == 'negative':\n",
    "        model_out_path = 'models/model_neg'\n",
    "    else:\n",
    "        model_out_path = None\n",
    "    return model_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, output_dir, n_iter=20, model=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(output_dir)\n",
    "        print(f\"Loaded model '{model}'\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        print(\"Created blank 'en' model\")\n",
    "        \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    # as a result, a pipe in nlp and we got ner to add labels\n",
    "        \n",
    "    for _, annotations in train_data:  # annotations : Dict\n",
    "        for ent in annotations.get('entities'):  # 'entities': [[start, end, 'selected_text']]\n",
    "            ner.add_label(ent[2])  # ent[2]: 'selected_text'\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        else:\n",
    "            nlp.resume_training()\n",
    "            \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=compounding(4, 500, 1.001))\n",
    "            # minibatch : given train_data, returns a generator yielding batches of the given size\n",
    "            # compounding(4, 500, 1.001):start 4, next 4 * 1.001, end 500\n",
    "            # in this example, size=4 and size=compounding(4, 500, 1.001) generates the same result\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # texts: a tuple of text of size 4 \n",
    "                # annotations: a tuple of dict of size 4\n",
    "                nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
    "            print('Losses', losses)\n",
    "    save_model(output_dir, nlp, 'st_ner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
