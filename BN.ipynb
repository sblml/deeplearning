{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalizatoiin vs Layer normalization  \n",
    "BN: across samples in a batch for each feature  \n",
    "LN: across all features in a sample\n",
    "    \n",
    "BN needs other samples in the batch  \n",
    "LN, however, only refers one sample, does not need any others\n",
    "\n",
    "in RNN, for (B, len_seq, n_features), LayerNorm(n_features) is required  \n",
    "LN should not refer other time steps and other samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization  \n",
    "https://wikidocs.net/61271\n",
    "\n",
    "https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, itâ€™s common terminology to call this Temporal Batch Normalization.\n",
    "\n",
    "Batchnorm1D(L or C)  \n",
    "given (N, L) over L dimension, mean and var for (:, l) l=1, ..., L  \n",
    "given (N, C, L) over C dimension, mean and var for (:, c, :) c=1, ... , C  \n",
    "i.e., L or C of statistics will be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchNorm layers in Pytorch have learable features  \n",
    "so that it is not provided as functional forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.blog.naver.com/laonple/220808903260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-does-the-batch-normalization-work-for-sequence-data/30839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalizations in FC, Conv, RNN are slightly different.  \n",
    "FC: (B, n_features), means are computed for (B, i) for i in n_features\n",
    "Conv: (B, C, H, W), means are computed for (B, i, H, W),  \n",
    "i.e. fixing channel i, normalize B samples  \n",
    "RNN: (B, T, n_features), means are computed for (B, t, n_features)\n",
    "features in different time steps should not be shared  \n",
    "fixing a time step, and normalize along the batch samples, e.g. the mean of the 10th features at t for B samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixing dimension is the input  \n",
    "For RNN, the time step is the fixing dimension, so the input will be seq_len  \n",
    "For CNN, the channel is the fixing dimension, so the input will be num_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalized along channel axis  \n",
    "normalized along feature axis or time axis  \n",
    "for BN, the input will be (B, along, ), the argument will be the length of along axis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
