{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT  \n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign a probability a given word to follow a sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Transformer module based on an attention mechanism to draw global dependencies between input and output.  \n",
    "The module is now highly modularized to  \n",
    "nn.TransformerEncoder, nn.TransformerEncoderLayer, nn.TransformerDecoder, nn.TrasnformerDecoderLayer\n",
    "\n",
    "We can simply use nn.Transformer the complete product,  \n",
    "or compose modularized components to customize the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.TransformerEncoder consists of multiple layers of nn.TransformerEncoderLayer.  \n",
    "https://pytorch.org/tutorials/_images/transformer_architecture.jpg  \n",
    "Nx nn.TransformerEncoderLayer in the picture form a TransformerEncoder\n",
    "\n",
    "nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)  \n",
    "with d_model=512, nhead=8, (10, 32, 512) -> (10, 32, 512) in the example  \n",
    "d_model : feature input\n",
    "nhead: the number of arrows to Multi-Head Attention in the picture  \n",
    "dim_feedforward  \n",
    "d_model determines the input and output shape\n",
    "\n",
    "nn.TransformerEncoder(encoder_layer, num_layers)  \n",
    "with encoder_layer=the layer above, num_layers=6, (10, 32, 512) -> (10, 32, 512) in the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the ordinary Attention the decoder attend the entire input  \n",
    "in self-attention the encoder refer the entire previous sequence, not just the previous  \n",
    "\n",
    "Along with the input sequence, a mask is required because the self-attention layers in nn.TransformerEncoder are only allowed to attend the earlier positions in the sequence.\n",
    "\n",
    "THINKING of the meaning of self-attention, attend the entire sequence before not just before the current  \n",
    "at t=10, given a seq of len t=20, attend [0: 9], not just [9], but [11:20] should not be accessed\n",
    "\n",
    "`tensor([[0., -inf, -inf],\n",
    "         [0., 0., -inf],\n",
    "         [0., 0., 0.]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE that Batch dimension is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate bsz batches by batchfy(data, bsz)\n",
    "bsz batches are further divided to be len_seq=bptt in get_batch()\n",
    "since too large len_seq 102499 of train_data will be impossible to train, reducing len_seq would be a remedy\n",
    "\n",
    "Note that slicing does not break the sequence relations\n",
    "we still have the correspondence between the target word and each input word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.ninp = ninp\n",
    "    \n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the len of div_term is max_len / 2  \n",
    "every even columns in pe filled by torch.sin  \n",
    "every odd columns in pe filled by torch.cos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x +  self.pe[:x.size(0), :]  # take the positional encoding up to len of x\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(position* div_term).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/text/stable/vocab.html  \n",
    "vocab.stoi(token) == vocab[token]  \n",
    "vocab[token] is the key for the token, an integer  \n",
    "vocab.stoi returns the entire dict  \n",
    "len(vocab.stoi) the size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)], dtype=torch.long) for item in raw_text_iter]\n",
    "    data = torch.cat(list(filter(lambda t: t.numel() > 0, data)))\n",
    "    return data\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbknl\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data is a text, the concatenate of train_iter  \n",
    "summed to one, and divided to be batchfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)  # along dim 0, from 0 len nbatch*bsz\n",
    "    data = data.view(bsz, -1).t().contiguous()  # batch dimension is 1\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source)-1-i)  # seq_len will be < 35 if the remaining < 35\n",
    "    data = source[i: i+seq_len]\n",
    "    target = source[i+1: i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "# batches are sliced to length of seq_len and returned\n",
    "# given [50, 20], returns [35, 20] as possible and [34, 20], [33, 20] ... [1, 20], [0, 20]\n",
    "# if len(source) == 102499, i=102498 returns []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate((range(0, train_data.size(0)-1, bptt))):  # i = 0, 35, 70, ...\n",
    "        data, targets = get_batch(train_data, i)  # shape of [35, 20], [700]\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequet_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        # output.view(-1, ntokens) -> (700, 28783), 700 guesses for the next tokens\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0.\n",
    "            start_time = time.time()\n",
    "\n",
    "            \n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) // bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = model.generate_square_subsequent_mask(bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20, 28783])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 28783])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(-1, ntokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 35,\n",
       " 70,\n",
       " 105,\n",
       " 140,\n",
       " 175,\n",
       " 210,\n",
       " 245,\n",
       " 280,\n",
       " 315,\n",
       " 350,\n",
       " 385,\n",
       " 420,\n",
       " 455,\n",
       " 490,\n",
       " 525,\n",
       " 560,\n",
       " 595,\n",
       " 630,\n",
       " 665,\n",
       " 700,\n",
       " 735,\n",
       " 770,\n",
       " 805,\n",
       " 840,\n",
       " 875,\n",
       " 910,\n",
       " 945,\n",
       " 980,\n",
       " 1015,\n",
       " 1050,\n",
       " 1085,\n",
       " 1120,\n",
       " 1155,\n",
       " 1190,\n",
       " 1225,\n",
       " 1260,\n",
       " 1295,\n",
       " 1330,\n",
       " 1365,\n",
       " 1400,\n",
       " 1435,\n",
       " 1470,\n",
       " 1505,\n",
       " 1540,\n",
       " 1575,\n",
       " 1610,\n",
       " 1645,\n",
       " 1680,\n",
       " 1715,\n",
       " 1750,\n",
       " 1785,\n",
       " 1820,\n",
       " 1855,\n",
       " 1890,\n",
       " 1925,\n",
       " 1960,\n",
       " 1995,\n",
       " 2030,\n",
       " 2065,\n",
       " 2100,\n",
       " 2135,\n",
       " 2170,\n",
       " 2205,\n",
       " 2240,\n",
       " 2275,\n",
       " 2310,\n",
       " 2345,\n",
       " 2380,\n",
       " 2415,\n",
       " 2450,\n",
       " 2485,\n",
       " 2520,\n",
       " 2555,\n",
       " 2590,\n",
       " 2625,\n",
       " 2660,\n",
       " 2695,\n",
       " 2730,\n",
       " 2765,\n",
       " 2800,\n",
       " 2835,\n",
       " 2870,\n",
       " 2905,\n",
       " 2940,\n",
       " 2975,\n",
       " 3010,\n",
       " 3045,\n",
       " 3080,\n",
       " 3115,\n",
       " 3150,\n",
       " 3185,\n",
       " 3220,\n",
       " 3255,\n",
       " 3290,\n",
       " 3325,\n",
       " 3360,\n",
       " 3395,\n",
       " 3430,\n",
       " 3465,\n",
       " 3500,\n",
       " 3535,\n",
       " 3570,\n",
       " 3605,\n",
       " 3640,\n",
       " 3675,\n",
       " 3710,\n",
       " 3745,\n",
       " 3780,\n",
       " 3815,\n",
       " 3850,\n",
       " 3885,\n",
       " 3920,\n",
       " 3955,\n",
       " 3990,\n",
       " 4025,\n",
       " 4060,\n",
       " 4095,\n",
       " 4130,\n",
       " 4165,\n",
       " 4200,\n",
       " 4235,\n",
       " 4270,\n",
       " 4305,\n",
       " 4340,\n",
       " 4375,\n",
       " 4410,\n",
       " 4445,\n",
       " 4480,\n",
       " 4515,\n",
       " 4550,\n",
       " 4585,\n",
       " 4620,\n",
       " 4655,\n",
       " 4690,\n",
       " 4725,\n",
       " 4760,\n",
       " 4795,\n",
       " 4830,\n",
       " 4865,\n",
       " 4900,\n",
       " 4935,\n",
       " 4970,\n",
       " 5005,\n",
       " 5040,\n",
       " 5075,\n",
       " 5110,\n",
       " 5145,\n",
       " 5180,\n",
       " 5215,\n",
       " 5250,\n",
       " 5285,\n",
       " 5320,\n",
       " 5355,\n",
       " 5390,\n",
       " 5425,\n",
       " 5460,\n",
       " 5495,\n",
       " 5530,\n",
       " 5565,\n",
       " 5600,\n",
       " 5635,\n",
       " 5670,\n",
       " 5705,\n",
       " 5740,\n",
       " 5775,\n",
       " 5810,\n",
       " 5845,\n",
       " 5880,\n",
       " 5915,\n",
       " 5950,\n",
       " 5985,\n",
       " 6020,\n",
       " 6055,\n",
       " 6090,\n",
       " 6125,\n",
       " 6160,\n",
       " 6195,\n",
       " 6230,\n",
       " 6265,\n",
       " 6300,\n",
       " 6335,\n",
       " 6370,\n",
       " 6405,\n",
       " 6440,\n",
       " 6475,\n",
       " 6510,\n",
       " 6545,\n",
       " 6580,\n",
       " 6615,\n",
       " 6650,\n",
       " 6685,\n",
       " 6720,\n",
       " 6755,\n",
       " 6790,\n",
       " 6825,\n",
       " 6860,\n",
       " 6895,\n",
       " 6930,\n",
       " 6965,\n",
       " 7000,\n",
       " 7035,\n",
       " 7070,\n",
       " 7105,\n",
       " 7140,\n",
       " 7175,\n",
       " 7210,\n",
       " 7245,\n",
       " 7280,\n",
       " 7315,\n",
       " 7350,\n",
       " 7385,\n",
       " 7420,\n",
       " 7455,\n",
       " 7490,\n",
       " 7525,\n",
       " 7560,\n",
       " 7595,\n",
       " 7630,\n",
       " 7665,\n",
       " 7700,\n",
       " 7735,\n",
       " 7770,\n",
       " 7805,\n",
       " 7840,\n",
       " 7875,\n",
       " 7910,\n",
       " 7945,\n",
       " 7980,\n",
       " 8015,\n",
       " 8050,\n",
       " 8085,\n",
       " 8120,\n",
       " 8155,\n",
       " 8190,\n",
       " 8225,\n",
       " 8260,\n",
       " 8295,\n",
       " 8330,\n",
       " 8365,\n",
       " 8400,\n",
       " 8435,\n",
       " 8470,\n",
       " 8505,\n",
       " 8540,\n",
       " 8575,\n",
       " 8610,\n",
       " 8645,\n",
       " 8680,\n",
       " 8715,\n",
       " 8750,\n",
       " 8785,\n",
       " 8820,\n",
       " 8855,\n",
       " 8890,\n",
       " 8925,\n",
       " 8960,\n",
       " 8995,\n",
       " 9030,\n",
       " 9065,\n",
       " 9100,\n",
       " 9135,\n",
       " 9170,\n",
       " 9205,\n",
       " 9240,\n",
       " 9275,\n",
       " 9310,\n",
       " 9345,\n",
       " 9380,\n",
       " 9415,\n",
       " 9450,\n",
       " 9485,\n",
       " 9520,\n",
       " 9555,\n",
       " 9590,\n",
       " 9625,\n",
       " 9660,\n",
       " 9695,\n",
       " 9730,\n",
       " 9765,\n",
       " 9800,\n",
       " 9835,\n",
       " 9870,\n",
       " 9905,\n",
       " 9940,\n",
       " 9975,\n",
       " 10010,\n",
       " 10045,\n",
       " 10080,\n",
       " 10115,\n",
       " 10150,\n",
       " 10185,\n",
       " 10220,\n",
       " 10255,\n",
       " 10290,\n",
       " 10325,\n",
       " 10360,\n",
       " 10395,\n",
       " 10430,\n",
       " 10465,\n",
       " 10500,\n",
       " 10535,\n",
       " 10570,\n",
       " 10605,\n",
       " 10640,\n",
       " 10675,\n",
       " 10710,\n",
       " 10745,\n",
       " 10780,\n",
       " 10815,\n",
       " 10850,\n",
       " 10885,\n",
       " 10920,\n",
       " 10955,\n",
       " 10990,\n",
       " 11025,\n",
       " 11060,\n",
       " 11095,\n",
       " 11130,\n",
       " 11165,\n",
       " 11200,\n",
       " 11235,\n",
       " 11270,\n",
       " 11305,\n",
       " 11340,\n",
       " 11375,\n",
       " 11410,\n",
       " 11445,\n",
       " 11480,\n",
       " 11515,\n",
       " 11550,\n",
       " 11585,\n",
       " 11620,\n",
       " 11655,\n",
       " 11690,\n",
       " 11725,\n",
       " 11760,\n",
       " 11795,\n",
       " 11830,\n",
       " 11865,\n",
       " 11900,\n",
       " 11935,\n",
       " 11970,\n",
       " 12005,\n",
       " 12040,\n",
       " 12075,\n",
       " 12110,\n",
       " 12145,\n",
       " 12180,\n",
       " 12215,\n",
       " 12250,\n",
       " 12285,\n",
       " 12320,\n",
       " 12355,\n",
       " 12390,\n",
       " 12425,\n",
       " 12460,\n",
       " 12495,\n",
       " 12530,\n",
       " 12565,\n",
       " 12600,\n",
       " 12635,\n",
       " 12670,\n",
       " 12705,\n",
       " 12740,\n",
       " 12775,\n",
       " 12810,\n",
       " 12845,\n",
       " 12880,\n",
       " 12915,\n",
       " 12950,\n",
       " 12985,\n",
       " 13020,\n",
       " 13055,\n",
       " 13090,\n",
       " 13125,\n",
       " 13160,\n",
       " 13195,\n",
       " 13230,\n",
       " 13265,\n",
       " 13300,\n",
       " 13335,\n",
       " 13370,\n",
       " 13405,\n",
       " 13440,\n",
       " 13475,\n",
       " 13510,\n",
       " 13545,\n",
       " 13580,\n",
       " 13615,\n",
       " 13650,\n",
       " 13685,\n",
       " 13720,\n",
       " 13755,\n",
       " 13790,\n",
       " 13825,\n",
       " 13860,\n",
       " 13895,\n",
       " 13930,\n",
       " 13965,\n",
       " 14000,\n",
       " 14035,\n",
       " 14070,\n",
       " 14105,\n",
       " 14140,\n",
       " 14175,\n",
       " 14210,\n",
       " 14245,\n",
       " 14280,\n",
       " 14315,\n",
       " 14350,\n",
       " 14385,\n",
       " 14420,\n",
       " 14455,\n",
       " 14490,\n",
       " 14525,\n",
       " 14560,\n",
       " 14595,\n",
       " 14630,\n",
       " 14665,\n",
       " 14700,\n",
       " 14735,\n",
       " 14770,\n",
       " 14805,\n",
       " 14840,\n",
       " 14875,\n",
       " 14910,\n",
       " 14945,\n",
       " 14980,\n",
       " 15015,\n",
       " 15050,\n",
       " 15085,\n",
       " 15120,\n",
       " 15155,\n",
       " 15190,\n",
       " 15225,\n",
       " 15260,\n",
       " 15295,\n",
       " 15330,\n",
       " 15365,\n",
       " 15400,\n",
       " 15435,\n",
       " 15470,\n",
       " 15505,\n",
       " 15540,\n",
       " 15575,\n",
       " 15610,\n",
       " 15645,\n",
       " 15680,\n",
       " 15715,\n",
       " 15750,\n",
       " 15785,\n",
       " 15820,\n",
       " 15855,\n",
       " 15890,\n",
       " 15925,\n",
       " 15960,\n",
       " 15995,\n",
       " 16030,\n",
       " 16065,\n",
       " 16100,\n",
       " 16135,\n",
       " 16170,\n",
       " 16205,\n",
       " 16240,\n",
       " 16275,\n",
       " 16310,\n",
       " 16345,\n",
       " 16380,\n",
       " 16415,\n",
       " 16450,\n",
       " 16485,\n",
       " 16520,\n",
       " 16555,\n",
       " 16590,\n",
       " 16625,\n",
       " 16660,\n",
       " 16695,\n",
       " 16730,\n",
       " 16765,\n",
       " 16800,\n",
       " 16835,\n",
       " 16870,\n",
       " 16905,\n",
       " 16940,\n",
       " 16975,\n",
       " 17010,\n",
       " 17045,\n",
       " 17080,\n",
       " 17115,\n",
       " 17150,\n",
       " 17185,\n",
       " 17220,\n",
       " 17255,\n",
       " 17290,\n",
       " 17325,\n",
       " 17360,\n",
       " 17395,\n",
       " 17430,\n",
       " 17465,\n",
       " 17500,\n",
       " 17535,\n",
       " 17570,\n",
       " 17605,\n",
       " 17640,\n",
       " 17675,\n",
       " 17710,\n",
       " 17745,\n",
       " 17780,\n",
       " 17815,\n",
       " 17850,\n",
       " 17885,\n",
       " 17920,\n",
       " 17955,\n",
       " 17990,\n",
       " 18025,\n",
       " 18060,\n",
       " 18095,\n",
       " 18130,\n",
       " 18165,\n",
       " 18200,\n",
       " 18235,\n",
       " 18270,\n",
       " 18305,\n",
       " 18340,\n",
       " 18375,\n",
       " 18410,\n",
       " 18445,\n",
       " 18480,\n",
       " 18515,\n",
       " 18550,\n",
       " 18585,\n",
       " 18620,\n",
       " 18655,\n",
       " 18690,\n",
       " 18725,\n",
       " 18760,\n",
       " 18795,\n",
       " 18830,\n",
       " 18865,\n",
       " 18900,\n",
       " 18935,\n",
       " 18970,\n",
       " 19005,\n",
       " 19040,\n",
       " 19075,\n",
       " 19110,\n",
       " 19145,\n",
       " 19180,\n",
       " 19215,\n",
       " 19250,\n",
       " 19285,\n",
       " 19320,\n",
       " 19355,\n",
       " 19390,\n",
       " 19425,\n",
       " 19460,\n",
       " 19495,\n",
       " 19530,\n",
       " 19565,\n",
       " 19600,\n",
       " 19635,\n",
       " 19670,\n",
       " 19705,\n",
       " 19740,\n",
       " 19775,\n",
       " 19810,\n",
       " 19845,\n",
       " 19880,\n",
       " 19915,\n",
       " 19950,\n",
       " 19985,\n",
       " 20020,\n",
       " 20055,\n",
       " 20090,\n",
       " 20125,\n",
       " 20160,\n",
       " 20195,\n",
       " 20230,\n",
       " 20265,\n",
       " 20300,\n",
       " 20335,\n",
       " 20370,\n",
       " 20405,\n",
       " 20440,\n",
       " 20475,\n",
       " 20510,\n",
       " 20545,\n",
       " 20580,\n",
       " 20615,\n",
       " 20650,\n",
       " 20685,\n",
       " 20720,\n",
       " 20755,\n",
       " 20790,\n",
       " 20825,\n",
       " 20860,\n",
       " 20895,\n",
       " 20930,\n",
       " 20965,\n",
       " 21000,\n",
       " 21035,\n",
       " 21070,\n",
       " 21105,\n",
       " 21140,\n",
       " 21175,\n",
       " 21210,\n",
       " 21245,\n",
       " 21280,\n",
       " 21315,\n",
       " 21350,\n",
       " 21385,\n",
       " 21420,\n",
       " 21455,\n",
       " 21490,\n",
       " 21525,\n",
       " 21560,\n",
       " 21595,\n",
       " 21630,\n",
       " 21665,\n",
       " 21700,\n",
       " 21735,\n",
       " 21770,\n",
       " 21805,\n",
       " 21840,\n",
       " 21875,\n",
       " 21910,\n",
       " 21945,\n",
       " 21980,\n",
       " 22015,\n",
       " 22050,\n",
       " 22085,\n",
       " 22120,\n",
       " 22155,\n",
       " 22190,\n",
       " 22225,\n",
       " 22260,\n",
       " 22295,\n",
       " 22330,\n",
       " 22365,\n",
       " 22400,\n",
       " 22435,\n",
       " 22470,\n",
       " 22505,\n",
       " 22540,\n",
       " 22575,\n",
       " 22610,\n",
       " 22645,\n",
       " 22680,\n",
       " 22715,\n",
       " 22750,\n",
       " 22785,\n",
       " 22820,\n",
       " 22855,\n",
       " 22890,\n",
       " 22925,\n",
       " 22960,\n",
       " 22995,\n",
       " 23030,\n",
       " 23065,\n",
       " 23100,\n",
       " 23135,\n",
       " 23170,\n",
       " 23205,\n",
       " 23240,\n",
       " 23275,\n",
       " 23310,\n",
       " 23345,\n",
       " 23380,\n",
       " 23415,\n",
       " 23450,\n",
       " 23485,\n",
       " 23520,\n",
       " 23555,\n",
       " 23590,\n",
       " 23625,\n",
       " 23660,\n",
       " 23695,\n",
       " 23730,\n",
       " 23765,\n",
       " 23800,\n",
       " 23835,\n",
       " 23870,\n",
       " 23905,\n",
       " 23940,\n",
       " 23975,\n",
       " 24010,\n",
       " 24045,\n",
       " 24080,\n",
       " 24115,\n",
       " 24150,\n",
       " 24185,\n",
       " 24220,\n",
       " 24255,\n",
       " 24290,\n",
       " 24325,\n",
       " 24360,\n",
       " 24395,\n",
       " 24430,\n",
       " 24465,\n",
       " 24500,\n",
       " 24535,\n",
       " 24570,\n",
       " 24605,\n",
       " 24640,\n",
       " 24675,\n",
       " 24710,\n",
       " 24745,\n",
       " 24780,\n",
       " 24815,\n",
       " 24850,\n",
       " 24885,\n",
       " 24920,\n",
       " 24955,\n",
       " 24990,\n",
       " 25025,\n",
       " 25060,\n",
       " 25095,\n",
       " 25130,\n",
       " 25165,\n",
       " 25200,\n",
       " 25235,\n",
       " 25270,\n",
       " 25305,\n",
       " 25340,\n",
       " 25375,\n",
       " 25410,\n",
       " 25445,\n",
       " 25480,\n",
       " 25515,\n",
       " 25550,\n",
       " 25585,\n",
       " 25620,\n",
       " 25655,\n",
       " 25690,\n",
       " 25725,\n",
       " 25760,\n",
       " 25795,\n",
       " 25830,\n",
       " 25865,\n",
       " 25900,\n",
       " 25935,\n",
       " 25970,\n",
       " 26005,\n",
       " 26040,\n",
       " 26075,\n",
       " 26110,\n",
       " 26145,\n",
       " 26180,\n",
       " 26215,\n",
       " 26250,\n",
       " 26285,\n",
       " 26320,\n",
       " 26355,\n",
       " 26390,\n",
       " 26425,\n",
       " 26460,\n",
       " 26495,\n",
       " 26530,\n",
       " 26565,\n",
       " 26600,\n",
       " 26635,\n",
       " 26670,\n",
       " 26705,\n",
       " 26740,\n",
       " 26775,\n",
       " 26810,\n",
       " 26845,\n",
       " 26880,\n",
       " 26915,\n",
       " 26950,\n",
       " 26985,\n",
       " 27020,\n",
       " 27055,\n",
       " 27090,\n",
       " 27125,\n",
       " 27160,\n",
       " 27195,\n",
       " 27230,\n",
       " 27265,\n",
       " 27300,\n",
       " 27335,\n",
       " 27370,\n",
       " 27405,\n",
       " 27440,\n",
       " 27475,\n",
       " 27510,\n",
       " 27545,\n",
       " 27580,\n",
       " 27615,\n",
       " 27650,\n",
       " 27685,\n",
       " 27720,\n",
       " 27755,\n",
       " 27790,\n",
       " 27825,\n",
       " 27860,\n",
       " 27895,\n",
       " 27930,\n",
       " 27965,\n",
       " 28000,\n",
       " 28035,\n",
       " 28070,\n",
       " 28105,\n",
       " 28140,\n",
       " 28175,\n",
       " 28210,\n",
       " 28245,\n",
       " 28280,\n",
       " 28315,\n",
       " 28350,\n",
       " 28385,\n",
       " 28420,\n",
       " 28455,\n",
       " 28490,\n",
       " 28525,\n",
       " 28560,\n",
       " 28595,\n",
       " 28630,\n",
       " 28665,\n",
       " 28700,\n",
       " 28735,\n",
       " 28770,\n",
       " 28805,\n",
       " 28840,\n",
       " 28875,\n",
       " 28910,\n",
       " 28945,\n",
       " 28980,\n",
       " 29015,\n",
       " 29050,\n",
       " 29085,\n",
       " 29120,\n",
       " 29155,\n",
       " 29190,\n",
       " 29225,\n",
       " 29260,\n",
       " 29295,\n",
       " 29330,\n",
       " 29365,\n",
       " 29400,\n",
       " 29435,\n",
       " 29470,\n",
       " 29505,\n",
       " 29540,\n",
       " 29575,\n",
       " 29610,\n",
       " 29645,\n",
       " 29680,\n",
       " 29715,\n",
       " 29750,\n",
       " 29785,\n",
       " 29820,\n",
       " 29855,\n",
       " 29890,\n",
       " 29925,\n",
       " 29960,\n",
       " 29995,\n",
       " 30030,\n",
       " 30065,\n",
       " 30100,\n",
       " 30135,\n",
       " 30170,\n",
       " 30205,\n",
       " 30240,\n",
       " 30275,\n",
       " 30310,\n",
       " 30345,\n",
       " 30380,\n",
       " 30415,\n",
       " 30450,\n",
       " 30485,\n",
       " 30520,\n",
       " 30555,\n",
       " 30590,\n",
       " 30625,\n",
       " 30660,\n",
       " 30695,\n",
       " 30730,\n",
       " 30765,\n",
       " 30800,\n",
       " 30835,\n",
       " 30870,\n",
       " 30905,\n",
       " 30940,\n",
       " 30975,\n",
       " 31010,\n",
       " 31045,\n",
       " 31080,\n",
       " 31115,\n",
       " 31150,\n",
       " 31185,\n",
       " 31220,\n",
       " 31255,\n",
       " 31290,\n",
       " 31325,\n",
       " 31360,\n",
       " 31395,\n",
       " 31430,\n",
       " 31465,\n",
       " 31500,\n",
       " 31535,\n",
       " 31570,\n",
       " 31605,\n",
       " 31640,\n",
       " 31675,\n",
       " 31710,\n",
       " 31745,\n",
       " 31780,\n",
       " 31815,\n",
       " 31850,\n",
       " 31885,\n",
       " 31920,\n",
       " 31955,\n",
       " 31990,\n",
       " 32025,\n",
       " 32060,\n",
       " 32095,\n",
       " 32130,\n",
       " 32165,\n",
       " 32200,\n",
       " 32235,\n",
       " 32270,\n",
       " 32305,\n",
       " 32340,\n",
       " 32375,\n",
       " 32410,\n",
       " 32445,\n",
       " 32480,\n",
       " 32515,\n",
       " 32550,\n",
       " 32585,\n",
       " 32620,\n",
       " 32655,\n",
       " 32690,\n",
       " 32725,\n",
       " 32760,\n",
       " 32795,\n",
       " 32830,\n",
       " 32865,\n",
       " 32900,\n",
       " 32935,\n",
       " 32970,\n",
       " 33005,\n",
       " 33040,\n",
       " 33075,\n",
       " 33110,\n",
       " 33145,\n",
       " 33180,\n",
       " 33215,\n",
       " 33250,\n",
       " 33285,\n",
       " 33320,\n",
       " 33355,\n",
       " 33390,\n",
       " 33425,\n",
       " 33460,\n",
       " 33495,\n",
       " 33530,\n",
       " 33565,\n",
       " 33600,\n",
       " 33635,\n",
       " 33670,\n",
       " 33705,\n",
       " 33740,\n",
       " 33775,\n",
       " 33810,\n",
       " 33845,\n",
       " 33880,\n",
       " 33915,\n",
       " 33950,\n",
       " 33985,\n",
       " 34020,\n",
       " 34055,\n",
       " 34090,\n",
       " 34125,\n",
       " 34160,\n",
       " 34195,\n",
       " 34230,\n",
       " 34265,\n",
       " 34300,\n",
       " 34335,\n",
       " 34370,\n",
       " 34405,\n",
       " 34440,\n",
       " 34475,\n",
       " 34510,\n",
       " 34545,\n",
       " 34580,\n",
       " 34615,\n",
       " 34650,\n",
       " 34685,\n",
       " 34720,\n",
       " 34755,\n",
       " 34790,\n",
       " 34825,\n",
       " 34860,\n",
       " 34895,\n",
       " 34930,\n",
       " 34965,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, train_data.size(0) - 1, bptt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102499, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fe6a2e89fa0>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'the': 2,\n",
       "             ',': 3,\n",
       "             '.': 4,\n",
       "             'of': 5,\n",
       "             'and': 6,\n",
       "             'in': 7,\n",
       "             'to': 8,\n",
       "             'a': 9,\n",
       "             '=': 10,\n",
       "             'was': 11,\n",
       "             \"'\": 12,\n",
       "             '@-@': 13,\n",
       "             'on': 14,\n",
       "             'as': 15,\n",
       "             's': 16,\n",
       "             'that': 17,\n",
       "             'for': 18,\n",
       "             'with': 19,\n",
       "             'by': 20,\n",
       "             ')': 21,\n",
       "             '(': 22,\n",
       "             '@': 23,\n",
       "             'is': 24,\n",
       "             'it': 25,\n",
       "             'from': 26,\n",
       "             'at': 27,\n",
       "             'his': 28,\n",
       "             'he': 29,\n",
       "             'were': 30,\n",
       "             'an': 31,\n",
       "             'had': 32,\n",
       "             'which': 33,\n",
       "             'be': 34,\n",
       "             'are': 35,\n",
       "             'this': 36,\n",
       "             'their': 37,\n",
       "             'first': 38,\n",
       "             'but': 39,\n",
       "             'not': 40,\n",
       "             '–': 41,\n",
       "             'one': 42,\n",
       "             'they': 43,\n",
       "             'its': 44,\n",
       "             'also': 45,\n",
       "             'after': 46,\n",
       "             'her': 47,\n",
       "             'or': 48,\n",
       "             'two': 49,\n",
       "             'have': 50,\n",
       "             'has': 51,\n",
       "             'been': 52,\n",
       "             'who': 53,\n",
       "             'she': 54,\n",
       "             'new': 55,\n",
       "             'other': 56,\n",
       "             'during': 57,\n",
       "             'when': 58,\n",
       "             'time': 59,\n",
       "             'all': 60,\n",
       "             'into': 61,\n",
       "             'more': 62,\n",
       "             'would': 63,\n",
       "             '1': 64,\n",
       "             'i': 65,\n",
       "             'over': 66,\n",
       "             'while': 67,\n",
       "             'game': 68,\n",
       "             'only': 69,\n",
       "             'most': 70,\n",
       "             '2': 71,\n",
       "             'three': 72,\n",
       "             'later': 73,\n",
       "             'about': 74,\n",
       "             'up': 75,\n",
       "             'may': 76,\n",
       "             'between': 77,\n",
       "             'him': 78,\n",
       "             'song': 79,\n",
       "             'there': 80,\n",
       "             'some': 81,\n",
       "             'than': 82,\n",
       "             'out': 83,\n",
       "             'no': 84,\n",
       "             'season': 85,\n",
       "             'year': 86,\n",
       "             'made': 87,\n",
       "             'city': 88,\n",
       "             '3': 89,\n",
       "             'such': 90,\n",
       "             'before': 91,\n",
       "             'where': 92,\n",
       "             'used': 93,\n",
       "             'series': 94,\n",
       "             'them': 95,\n",
       "             'second': 96,\n",
       "             'world': 97,\n",
       "             'being': 98,\n",
       "             'years': 99,\n",
       "             'both': 100,\n",
       "             '000': 101,\n",
       "             'many': 102,\n",
       "             'these': 103,\n",
       "             'film': 104,\n",
       "             'however': 105,\n",
       "             'album': 106,\n",
       "             'south': 107,\n",
       "             'war': 108,\n",
       "             'through': 109,\n",
       "             '5': 110,\n",
       "             'north': 111,\n",
       "             'then': 112,\n",
       "             'can': 113,\n",
       "             'part': 114,\n",
       "             'early': 115,\n",
       "             'several': 116,\n",
       "             '4': 117,\n",
       "             'number': 118,\n",
       "             'state': 119,\n",
       "             'including': 120,\n",
       "             'against': 121,\n",
       "             'well': 122,\n",
       "             '/': 123,\n",
       "             'known': 124,\n",
       "             'became': 125,\n",
       "             '—': 126,\n",
       "             'm': 127,\n",
       "             'four': 128,\n",
       "             'united': 129,\n",
       "             'under': 130,\n",
       "             'although': 131,\n",
       "             'century': 132,\n",
       "             'day': 133,\n",
       "             'following': 134,\n",
       "             'music': 135,\n",
       "             'began': 136,\n",
       "             'because': 137,\n",
       "             'so': 138,\n",
       "             'work': 139,\n",
       "             'like': 140,\n",
       "             'end': 141,\n",
       "             'called': 142,\n",
       "             'episode': 143,\n",
       "             'until': 144,\n",
       "             'found': 145,\n",
       "             'said': 146,\n",
       "             'area': 147,\n",
       "             'could': 148,\n",
       "             'states': 149,\n",
       "             'american': 150,\n",
       "             'people': 151,\n",
       "             '6': 152,\n",
       "             'since': 153,\n",
       "             'british': 154,\n",
       "             'each': 155,\n",
       "             'released': 156,\n",
       "             'same': 157,\n",
       "             'team': 158,\n",
       "             'church': 159,\n",
       "             '10': 160,\n",
       "             'around': 161,\n",
       "             'long': 162,\n",
       "             'did': 163,\n",
       "             'along': 164,\n",
       "             'million': 165,\n",
       "             'five': 166,\n",
       "             'life': 167,\n",
       "             'national': 168,\n",
       "             '0': 169,\n",
       "             'back': 170,\n",
       "             'john': 171,\n",
       "             'high': 172,\n",
       "             'company': 173,\n",
       "             't': 174,\n",
       "             'another': 175,\n",
       "             'best': 176,\n",
       "             'use': 177,\n",
       "             '%': 178,\n",
       "             'you': 179,\n",
       "             'if': 180,\n",
       "             'final': 181,\n",
       "             'september': 182,\n",
       "             'august': 183,\n",
       "             'river': 184,\n",
       "             'large': 185,\n",
       "             'what': 186,\n",
       "             'west': 187,\n",
       "             '8': 188,\n",
       "             'km': 189,\n",
       "             'off': 190,\n",
       "             'down': 191,\n",
       "             '7': 192,\n",
       "             'due': 193,\n",
       "             'games': 194,\n",
       "             'june': 195,\n",
       "             'line': 196,\n",
       "             'history': 197,\n",
       "             'will': 198,\n",
       "             'name': 199,\n",
       "             'now': 200,\n",
       "             'any': 201,\n",
       "             'storm': 202,\n",
       "             'home': 203,\n",
       "             'received': 204,\n",
       "             '9': 205,\n",
       "             'described': 206,\n",
       "             'government': 207,\n",
       "             'six': 208,\n",
       "             'species': 209,\n",
       "             'within': 210,\n",
       "             'much': 211,\n",
       "             'group': 212,\n",
       "             'family': 213,\n",
       "             'october': 214,\n",
       "             'played': 215,\n",
       "             '$': 216,\n",
       "             'east': 217,\n",
       "             'league': 218,\n",
       "             'general': 219,\n",
       "             'set': 220,\n",
       "             'took': 221,\n",
       "             '[': 222,\n",
       "             ']': 223,\n",
       "             'major': 224,\n",
       "             'road': 225,\n",
       "             'july': 226,\n",
       "             'wrote': 227,\n",
       "             'late': 228,\n",
       "             'single': 229,\n",
       "             'won': 230,\n",
       "             'system': 231,\n",
       "             'play': 232,\n",
       "             'video': 233,\n",
       "             'times': 234,\n",
       "             'us': 235,\n",
       "             'according': 236,\n",
       "             'record': 237,\n",
       "             'third': 238,\n",
       "             'based': 239,\n",
       "             'april': 240,\n",
       "             'man': 241,\n",
       "             'included': 242,\n",
       "             'just': 243,\n",
       "             'march': 244,\n",
       "             'book': 245,\n",
       "             'those': 246,\n",
       "             'january': 247,\n",
       "             'show': 248,\n",
       "             'named': 249,\n",
       "             'even': 250,\n",
       "             'very': 251,\n",
       "             'england': 252,\n",
       "             'main': 253,\n",
       "             'white': 254,\n",
       "             'left': 255,\n",
       "             'york': 256,\n",
       "             'men': 257,\n",
       "             'school': 258,\n",
       "             'small': 259,\n",
       "             'though': 260,\n",
       "             'division': 261,\n",
       "             'club': 262,\n",
       "             'way': 263,\n",
       "             'old': 264,\n",
       "             'original': 265,\n",
       "             'near': 266,\n",
       "             'last': 267,\n",
       "             '12': 268,\n",
       "             'november': 269,\n",
       "             'water': 270,\n",
       "             'death': 271,\n",
       "             'place': 272,\n",
       "             '20': 273,\n",
       "             '15': 274,\n",
       "             'tropical': 275,\n",
       "             'december': 276,\n",
       "             'built': 277,\n",
       "             'own': 278,\n",
       "             'character': 279,\n",
       "             'we': 280,\n",
       "             'songs': 281,\n",
       "             'top': 282,\n",
       "             'de': 283,\n",
       "             'form': 284,\n",
       "             '30': 285,\n",
       "             'player': 286,\n",
       "             'do': 287,\n",
       "             'king': 288,\n",
       "             'black': 289,\n",
       "             'public': 290,\n",
       "             'german': 291,\n",
       "             'island': 292,\n",
       "             'next': 293,\n",
       "             '2009': 294,\n",
       "             'make': 295,\n",
       "             '2008': 296,\n",
       "             'still': 297,\n",
       "             '2010': 298,\n",
       "             'u': 299,\n",
       "             'role': 300,\n",
       "             'led': 301,\n",
       "             'again': 302,\n",
       "             'moved': 303,\n",
       "             'career': 304,\n",
       "             'ii': 305,\n",
       "             'university': 306,\n",
       "             'without': 307,\n",
       "             'love': 308,\n",
       "             'often': 309,\n",
       "             'among': 310,\n",
       "             'recorded': 311,\n",
       "             'further': 312,\n",
       "             'hurricane': 313,\n",
       "             'military': 314,\n",
       "             'period': 315,\n",
       "             'star': 316,\n",
       "             'local': 317,\n",
       "             'considered': 318,\n",
       "             'army': 319,\n",
       "             'production': 320,\n",
       "             'release': 321,\n",
       "             'side': 322,\n",
       "             '2007': 323,\n",
       "             'great': 324,\n",
       "             'house': 325,\n",
       "             'came': 326,\n",
       "             'published': 327,\n",
       "             'written': 328,\n",
       "             '100': 329,\n",
       "             'continued': 330,\n",
       "             'power': 331,\n",
       "             'town': 332,\n",
       "             'english': 333,\n",
       "             'story': 334,\n",
       "             'days': 335,\n",
       "             'forces': 336,\n",
       "             'run': 337,\n",
       "             'held': 338,\n",
       "             'route': 339,\n",
       "             'french': 340,\n",
       "             'support': 341,\n",
       "             '14': 342,\n",
       "             '16': 343,\n",
       "             '11': 344,\n",
       "             'force': 345,\n",
       "             'half': 346,\n",
       "             'few': 347,\n",
       "             'take': 348,\n",
       "             'international': 349,\n",
       "             'having': 350,\n",
       "             '25': 351,\n",
       "             'county': 352,\n",
       "             'land': 353,\n",
       "             'throughout': 354,\n",
       "             '2011': 355,\n",
       "             'point': 356,\n",
       "             '18': 357,\n",
       "             'become': 358,\n",
       "             '2006': 359,\n",
       "             'children': 360,\n",
       "             'order': 361,\n",
       "             'light': 362,\n",
       "             'version': 363,\n",
       "             'title': 364,\n",
       "             'former': 365,\n",
       "             'lost': 366,\n",
       "             'track': 367,\n",
       "             'different': 368,\n",
       "             '&': 369,\n",
       "             'development': 370,\n",
       "             'field': 371,\n",
       "             'ship': 372,\n",
       "             'similar': 373,\n",
       "             'despite': 374,\n",
       "             'live': 375,\n",
       "             'common': 376,\n",
       "             'members': 377,\n",
       "             'park': 378,\n",
       "             'c': 379,\n",
       "             'february': 380,\n",
       "             '13': 381,\n",
       "             'gave': 382,\n",
       "             'produced': 383,\n",
       "             'short': 384,\n",
       "             'southern': 385,\n",
       "             '!': 386,\n",
       "             'dylan': 387,\n",
       "             'little': 388,\n",
       "             'site': 389,\n",
       "             '2012': 390,\n",
       "             'once': 391,\n",
       "             'television': 392,\n",
       "             'writing': 393,\n",
       "             'given': 394,\n",
       "             'central': 395,\n",
       "             'control': 396,\n",
       "             'total': 397,\n",
       "             'band': 398,\n",
       "             'country': 399,\n",
       "             'service': 400,\n",
       "             'northern': 401,\n",
       "             're': 402,\n",
       "             'include': 403,\n",
       "             'young': 404,\n",
       "             'fire': 405,\n",
       "             'position': 406,\n",
       "             'battalion': 407,\n",
       "             'making': 408,\n",
       "             'never': 409,\n",
       "             'away': 410,\n",
       "             'seven': 411,\n",
       "             'tour': 412,\n",
       "             'age': 413,\n",
       "             'air': 414,\n",
       "             'lead': 415,\n",
       "             '2013': 416,\n",
       "             'how': 417,\n",
       "             'open': 418,\n",
       "             'reported': 419,\n",
       "             'seen': 420,\n",
       "             'battle': 421,\n",
       "             'highway': 422,\n",
       "             'eastern': 423,\n",
       "             'good': 424,\n",
       "             'western': 425,\n",
       "             'stated': 426,\n",
       "             'attack': 427,\n",
       "             'red': 428,\n",
       "             'god': 429,\n",
       "             'h': 430,\n",
       "             'match': 431,\n",
       "             'across': 432,\n",
       "             'st': 433,\n",
       "             'body': 434,\n",
       "             'instead': 435,\n",
       "             'returned': 436,\n",
       "             'ships': 437,\n",
       "             'established': 438,\n",
       "             'using': 439,\n",
       "             'ft': 440,\n",
       "             'population': 441,\n",
       "             'america': 442,\n",
       "             'construction': 443,\n",
       "             'modern': 444,\n",
       "             'week': 445,\n",
       "             'noted': 446,\n",
       "             'less': 447,\n",
       "             'my': 448,\n",
       "             'royal': 449,\n",
       "             'head': 450,\n",
       "             'reached': 451,\n",
       "             'building': 452,\n",
       "             'developed': 453,\n",
       "             'eight': 454,\n",
       "             'rock': 455,\n",
       "             'ireland': 456,\n",
       "             'players': 457,\n",
       "             'brigade': 458,\n",
       "             'b': 459,\n",
       "             'president': 460,\n",
       "             'result': 461,\n",
       "             'thought': 462,\n",
       "             'performance': 463,\n",
       "             'right': 464,\n",
       "             'london': 465,\n",
       "             'miles': 466,\n",
       "             'himself': 467,\n",
       "             'father': 468,\n",
       "             'per': 469,\n",
       "             'important': 470,\n",
       "             'style': 471,\n",
       "             'performed': 472,\n",
       "             'felt': 473,\n",
       "             'australia': 474,\n",
       "             'various': 475,\n",
       "             '17': 476,\n",
       "             'full': 477,\n",
       "             'areas': 478,\n",
       "             'feet': 479,\n",
       "             'previous': 480,\n",
       "             'events': 481,\n",
       "             'win': 482,\n",
       "             'low': 483,\n",
       "             'died': 484,\n",
       "             'kingdom': 485,\n",
       "             'guitar': 486,\n",
       "             'football': 487,\n",
       "             'others': 488,\n",
       "             'art': 489,\n",
       "             'mm': 490,\n",
       "             'originally': 491,\n",
       "             'project': 492,\n",
       "             'too': 493,\n",
       "             'went': 494,\n",
       "             'human': 495,\n",
       "             '23': 496,\n",
       "             'level': 497,\n",
       "             'upon': 498,\n",
       "             'range': 499,\n",
       "             'works': 500,\n",
       "             'formed': 501,\n",
       "             'started': 502,\n",
       "             'characters': 503,\n",
       "             'james': 504,\n",
       "             'political': 505,\n",
       "             'women': 506,\n",
       "             'should': 507,\n",
       "             'cup': 508,\n",
       "             'port': 509,\n",
       "             '50': 510,\n",
       "             'caused': 511,\n",
       "             '21': 512,\n",
       "             '28': 513,\n",
       "             'eventually': 514,\n",
       "             'located': 515,\n",
       "             '19': 516,\n",
       "             '24': 517,\n",
       "             'stars': 518,\n",
       "             'critics': 519,\n",
       "             'ground': 520,\n",
       "             'sent': 521,\n",
       "             'able': 522,\n",
       "             'created': 523,\n",
       "             '2004': 524,\n",
       "             'me': 525,\n",
       "             '2005': 526,\n",
       "             'class': 527,\n",
       "             'd': 528,\n",
       "             'chart': 529,\n",
       "             'night': 530,\n",
       "             'born': 531,\n",
       "             'region': 532,\n",
       "             'street': 533,\n",
       "             'center': 534,\n",
       "             'court': 535,\n",
       "             'design': 536,\n",
       "             'together': 537,\n",
       "             'director': 538,\n",
       "             'popular': 539,\n",
       "             'present': 540,\n",
       "             'strong': 541,\n",
       "             'award': 542,\n",
       "             'every': 543,\n",
       "             '’': 544,\n",
       "             'return': 545,\n",
       "             'son': 546,\n",
       "             'hero': 547,\n",
       "             'remained': 548,\n",
       "             'see': 549,\n",
       "             'completed': 550,\n",
       "             'guns': 551,\n",
       "             'novel': 552,\n",
       "             'scored': 553,\n",
       "             'announced': 554,\n",
       "             'australian': 555,\n",
       "             'grand': 556,\n",
       "             '22': 557,\n",
       "             'almost': 558,\n",
       "             'fourth': 559,\n",
       "             'behind': 560,\n",
       "             'damage': 561,\n",
       "             'least': 562,\n",
       "             '26': 563,\n",
       "             'brown': 564,\n",
       "             'party': 565,\n",
       "             'ten': 566,\n",
       "             'added': 567,\n",
       "             'heavy': 568,\n",
       "             'followed': 569,\n",
       "             'months': 570,\n",
       "             'appeared': 571,\n",
       "             'wife': 572,\n",
       "             'killed': 573,\n",
       "             'addition': 574,\n",
       "             'does': 575,\n",
       "             'playing': 576,\n",
       "             'success': 577,\n",
       "             'awards': 578,\n",
       "             'list': 579,\n",
       "             'features': 580,\n",
       "             'aircraft': 581,\n",
       "             '2003': 582,\n",
       "             'coast': 583,\n",
       "             'sea': 584,\n",
       "             'taken': 585,\n",
       "             '2015': 586,\n",
       "             'david': 587,\n",
       "             'leading': 588,\n",
       "             'championship': 589,\n",
       "             'action': 590,\n",
       "             'france': 591,\n",
       "             'either': 592,\n",
       "             'europe': 593,\n",
       "             'front': 594,\n",
       "             'recording': 595,\n",
       "             'served': 596,\n",
       "             'towards': 597,\n",
       "             'campaign': 598,\n",
       "             'operations': 599,\n",
       "             'gold': 600,\n",
       "             'mother': 601,\n",
       "             'put': 602,\n",
       "             'decided': 603,\n",
       "             'elements': 604,\n",
       "             'close': 605,\n",
       "             'records': 606,\n",
       "             'believed': 607,\n",
       "             'fleet': 608,\n",
       "             'generally': 609,\n",
       "             'magazine': 610,\n",
       "             'carey': 611,\n",
       "             'ever': 612,\n",
       "             'female': 613,\n",
       "             'post': 614,\n",
       "             'poem': 615,\n",
       "             'sold': 616,\n",
       "             'soon': 617,\n",
       "             'example': 618,\n",
       "             'infantry': 619,\n",
       "             'points': 620,\n",
       "             'significant': 621,\n",
       "             'fort': 622,\n",
       "             'goal': 623,\n",
       "             'move': 624,\n",
       "             'weeks': 625,\n",
       "             'rather': 626,\n",
       "             'study': 627,\n",
       "             'european': 628,\n",
       "             'federer': 629,\n",
       "             'outside': 630,\n",
       "             'o': 631,\n",
       "             'opened': 632,\n",
       "             'robert': 633,\n",
       "             'case': 634,\n",
       "             'directed': 635,\n",
       "             'brought': 636,\n",
       "             'help': 637,\n",
       "             'law': 638,\n",
       "             'non': 639,\n",
       "             'finished': 640,\n",
       "             '27': 641,\n",
       "             'earlier': 642,\n",
       "             'wales': 643,\n",
       "             'william': 644,\n",
       "             'featured': 645,\n",
       "             'go': 646,\n",
       "             'get': 647,\n",
       "             'victory': 648,\n",
       "             'manager': 649,\n",
       "             'successful': 650,\n",
       "             'act': 651,\n",
       "             'gun': 652,\n",
       "             'stage': 653,\n",
       "             'association': 654,\n",
       "             'member': 655,\n",
       "             'provided': 656,\n",
       "             'mi': 657,\n",
       "             'mid': 658,\n",
       "             'opening': 659,\n",
       "             'start': 660,\n",
       "             'village': 661,\n",
       "             'working': 662,\n",
       "             'council': 663,\n",
       "             'wanted': 664,\n",
       "             'appearance': 665,\n",
       "             'jordan': 666,\n",
       "             'particularly': 667,\n",
       "             'roman': 668,\n",
       "             'troops': 669,\n",
       "             '2014': 670,\n",
       "             '40': 671,\n",
       "             'atlantic': 672,\n",
       "             'depression': 673,\n",
       "             'initially': 674,\n",
       "             'tech': 675,\n",
       "             '29': 676,\n",
       "             'evidence': 677,\n",
       "             'yard': 678,\n",
       "             'far': 679,\n",
       "             'find': 680,\n",
       "             'largest': 681,\n",
       "             'office': 682,\n",
       "             'blue': 683,\n",
       "             'dam': 684,\n",
       "             'george': 685,\n",
       "             'review': 686,\n",
       "             'attempt': 687,\n",
       "             'possible': 688,\n",
       "             'saw': 689,\n",
       "             'special': 690,\n",
       "             'type': 691,\n",
       "             'month': 692,\n",
       "             'summer': 693,\n",
       "             '19th': 694,\n",
       "             'above': 695,\n",
       "             'union': 696,\n",
       "             'yards': 697,\n",
       "             'e': 698,\n",
       "             'florida': 699,\n",
       "             'rest': 700,\n",
       "             'allowed': 701,\n",
       "             'event': 702,\n",
       "             'race': 703,\n",
       "             'winds': 704,\n",
       "             'critical': 705,\n",
       "             'saying': 706,\n",
       "             'creek': 707,\n",
       "             'cross': 708,\n",
       "             'hours': 709,\n",
       "             'whom': 710,\n",
       "             'nine': 711,\n",
       "             '2001': 712,\n",
       "             'missouri': 713,\n",
       "             'plan': 714,\n",
       "             'police': 715,\n",
       "             'worked': 716,\n",
       "             'community': 717,\n",
       "             'designed': 718,\n",
       "             'reception': 719,\n",
       "             'society': 720,\n",
       "             '500': 721,\n",
       "             'previously': 722,\n",
       "             'free': 723,\n",
       "             'forced': 724,\n",
       "             'middle': 725,\n",
       "             'process': 726,\n",
       "             'era': 727,\n",
       "             'operation': 728,\n",
       "             'radio': 729,\n",
       "             'real': 730,\n",
       "             'remains': 731,\n",
       "             '200': 732,\n",
       "             'increased': 733,\n",
       "             'official': 734,\n",
       "             'praised': 735,\n",
       "             'research': 736,\n",
       "             'hall': 737,\n",
       "             'lower': 738,\n",
       "             'parliament': 739,\n",
       "             'station': 740,\n",
       "             'come': 741,\n",
       "             'michael': 742,\n",
       "             'relationship': 743,\n",
       "             'command': 744,\n",
       "             'commander': 745,\n",
       "             'hill': 746,\n",
       "             'regiment': 747,\n",
       "             'studio': 748,\n",
       "             'units': 749,\n",
       "             'base': 750,\n",
       "             'taking': 751,\n",
       "             '£': 752,\n",
       "             'parts': 753,\n",
       "             'replaced': 754,\n",
       "             'writer': 755,\n",
       "             'ball': 756,\n",
       "             'industry': 757,\n",
       "             'media': 758,\n",
       "             'navy': 759,\n",
       "             'social': 760,\n",
       "             'food': 761,\n",
       "             'r': 762,\n",
       "             'bay': 763,\n",
       "             'co': 764,\n",
       "             'college': 765,\n",
       "             'highest': 766,\n",
       "             'reviews': 767,\n",
       "             'beginning': 768,\n",
       "             'claimed': 769,\n",
       "             'don': 770,\n",
       "             '°': 771,\n",
       "             'canada': 772,\n",
       "             'estimated': 773,\n",
       "             'mph': 774,\n",
       "             'museum': 775,\n",
       "             'section': 776,\n",
       "             'goals': 777,\n",
       "             'stone': 778,\n",
       "             'average': 779,\n",
       "             'commercial': 780,\n",
       "             'japanese': 781,\n",
       "             'joined': 782,\n",
       "             'l': 783,\n",
       "             'religious': 784,\n",
       "             'involved': 785,\n",
       "             'oldham': 786,\n",
       "             'placed': 787,\n",
       "             'stories': 788,\n",
       "             'training': 789,\n",
       "             'introduced': 790,\n",
       "             'met': 791,\n",
       "             'shot': 792,\n",
       "             'signed': 793,\n",
       "             'suggested': 794,\n",
       "             'lines': 795,\n",
       "             'sometimes': 796,\n",
       "             '31': 797,\n",
       "             'background': 798,\n",
       "             'business': 799,\n",
       "             'face': 800,\n",
       "             'olivier': 801,\n",
       "             'paul': 802,\n",
       "             'today': 803,\n",
       "             'complete': 804,\n",
       "             'going': 805,\n",
       "             'itself': 806,\n",
       "             'scene': 807,\n",
       "             'henry': 808,\n",
       "             'mexico': 809,\n",
       "             'structure': 810,\n",
       "             'additional': 811,\n",
       "             'available': 812,\n",
       "             'give': 813,\n",
       "             'thus': 814,\n",
       "             'cast': 815,\n",
       "             'date': 816,\n",
       "             'horse': 817,\n",
       "             'language': 818,\n",
       "             'loss': 819,\n",
       "             'india': 820,\n",
       "             'nearly': 821,\n",
       "             'sound': 822,\n",
       "             'term': 823,\n",
       "             'whose': 824,\n",
       "             'fifth': 825,\n",
       "             'past': 826,\n",
       "             'thomas': 827,\n",
       "             'approximately': 828,\n",
       "             'indian': 829,\n",
       "             'irish': 830,\n",
       "             'must': 831,\n",
       "             'program': 832,\n",
       "             'already': 833,\n",
       "             'appointed': 834,\n",
       "             'capital': 835,\n",
       "             'entire': 836,\n",
       "             'friends': 837,\n",
       "             'britain': 838,\n",
       "             'damaged': 839,\n",
       "             'native': 840,\n",
       "             'prior': 841,\n",
       "             'shows': 842,\n",
       "             'told': 843,\n",
       "             'forest': 844,\n",
       "             'issue': 845,\n",
       "             'mark': 846,\n",
       "             'names': 847,\n",
       "             'probably': 848,\n",
       "             'turned': 849,\n",
       "             'male': 850,\n",
       "             'sun': 851,\n",
       "             'winning': 852,\n",
       "             '-': 853,\n",
       "             'change': 854,\n",
       "             'g': 855,\n",
       "             'our': 856,\n",
       "             'students': 857,\n",
       "             '?': 858,\n",
       "             'earth': 859,\n",
       "             'length': 860,\n",
       "             'passed': 861,\n",
       "             'size': 862,\n",
       "             'child': 863,\n",
       "             'civil': 864,\n",
       "             'especially': 865,\n",
       "             'christian': 866,\n",
       "             'enough': 867,\n",
       "             'notes': 868,\n",
       "             'woman': 869,\n",
       "             'chinese': 870,\n",
       "             'failed': 871,\n",
       "             'forward': 872,\n",
       "             'mixed': 873,\n",
       "             'overall': 874,\n",
       "             'running': 875,\n",
       "             'better': 876,\n",
       "             'captain': 877,\n",
       "             'ny': 878,\n",
       "             '2002': 879,\n",
       "             'limited': 880,\n",
       "             'future': 881,\n",
       "             'iii': 882,\n",
       "             'minor': 883,\n",
       "             'network': 884,\n",
       "             'arrived': 885,\n",
       "             'changes': 886,\n",
       "             'includes': 887,\n",
       "             'might': 888,\n",
       "             'moving': 889,\n",
       "             'ordered': 890,\n",
       "             'pacific': 891,\n",
       "             'regular': 892,\n",
       "             'spent': 893,\n",
       "             'wheeler': 894,\n",
       "             'canadian': 895,\n",
       "             'cathedral': 896,\n",
       "             'education': 897,\n",
       "             'larger': 898,\n",
       "             'remaining': 899,\n",
       "             'usually': 900,\n",
       "             'birds': 901,\n",
       "             'department': 902,\n",
       "             'hand': 903,\n",
       "             'hit': 904,\n",
       "             'lake': 905,\n",
       "             'required': 906,\n",
       "             'san': 907,\n",
       "             'uk': 908,\n",
       "             'decision': 909,\n",
       "             'latter': 910,\n",
       "             'africa': 911,\n",
       "             'plot': 912,\n",
       "             'response': 913,\n",
       "             '2000': 914,\n",
       "             'musical': 915,\n",
       "             'round': 916,\n",
       "             'space': 917,\n",
       "             'voice': 918,\n",
       "             'wide': 919,\n",
       "             'appear': 920,\n",
       "             'crew': 921,\n",
       "             'debut': 922,\n",
       "             'groups': 923,\n",
       "             'mounted': 924,\n",
       "             'related': 925,\n",
       "             'centre': 926,\n",
       "             'jin': 927,\n",
       "             'rachel': 928,\n",
       "             'territory': 929,\n",
       "             'view': 930,\n",
       "             '00': 931,\n",
       "             'billboard': 932,\n",
       "             'ended': 933,\n",
       "             'feature': 934,\n",
       "             'films': 935,\n",
       "             'nature': 936,\n",
       "             'positive': 937,\n",
       "             'saint': 938,\n",
       "             'science': 939,\n",
       "             'culture': 940,\n",
       "             'finally': 941,\n",
       "             'flight': 942,\n",
       "             'score': 943,\n",
       "             'squadron': 944,\n",
       "             'supported': 945,\n",
       "             'becoming': 946,\n",
       "             'money': 947,\n",
       "             'pressure': 948,\n",
       "             'always': 949,\n",
       "             'books': 950,\n",
       "             'charles': 951,\n",
       "             'provide': 952,\n",
       "             'smaller': 953,\n",
       "             '1995': 954,\n",
       "             'anti': 955,\n",
       "             'discovered': 956,\n",
       "             'private': 957,\n",
       "             'shown': 958,\n",
       "             'board': 959,\n",
       "             'minutes': 960,\n",
       "             'particular': 961,\n",
       "             'shortly': 962,\n",
       "             'defeated': 963,\n",
       "             'difficult': 964,\n",
       "             'experience': 965,\n",
       "             'mass': 966,\n",
       "             'nations': 967,\n",
       "             'person': 968,\n",
       "             'peter': 969,\n",
       "             'temple': 970,\n",
       "             'trade': 971,\n",
       "             'big': 972,\n",
       "             'staff': 973,\n",
       "             'subsequently': 974,\n",
       "             'surface': 975,\n",
       "             'effects': 976,\n",
       "             'japan': 977,\n",
       "             'lack': 978,\n",
       "             'living': 979,\n",
       "             'press': 980,\n",
       "             'upper': 981,\n",
       "             'zealand': 982,\n",
       "             'professional': 983,\n",
       "             'word': 984,\n",
       "             'fact': 985,\n",
       "             'greater': 986,\n",
       "             'material': 987,\n",
       "             'tv': 988,\n",
       "             '60': 989,\n",
       "             'problems': 990,\n",
       "             'room': 991,\n",
       "             'self': 992,\n",
       "             'teams': 993,\n",
       "             'bridge': 994,\n",
       "             'collection': 995,\n",
       "             'cut': 996,\n",
       "             'gods': 997,\n",
       "             'idea': 998,\n",
       "             'intended': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-01d9622ec486>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batchify(torch.tensor(torch.arange(100)), 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 20, 40, 60, 80],\n",
       "        [ 1, 21, 41, 61, 81],\n",
       "        [ 2, 22, 42, 62, 82],\n",
       "        [ 3, 23, 43, 63, 83],\n",
       "        [ 4, 24, 44, 64, 84],\n",
       "        [ 5, 25, 45, 65, 85],\n",
       "        [ 6, 26, 46, 66, 86],\n",
       "        [ 7, 27, 47, 67, 87],\n",
       "        [ 8, 28, 48, 68, 88],\n",
       "        [ 9, 29, 49, 69, 89],\n",
       "        [10, 30, 50, 70, 90],\n",
       "        [11, 31, 51, 71, 91],\n",
       "        [12, 32, 52, 72, 92],\n",
       "        [13, 33, 53, 73, 93],\n",
       "        [14, 34, 54, 74, 94],\n",
       "        [15, 35, 55, 75, 95],\n",
       "        [16, 36, 56, 76, 96],\n",
       "        [17, 37, 57, 77, 97],\n",
       "        [18, 38, 58, 78, 98],\n",
       "        [19, 39, 59, 79, 99]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchify(torch.tensor(torch.arange(100)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2049990])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((filter(lambda t: t > 0, [-1, 0, 1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f99489e167f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "tuple(filter(lambda t: t.numel() > 0, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'=': 4, 'gameplay': 1})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fe6a2e89160>>,\n",
       "            {'<unk>': 0, '<pad>': 1, '=': 2, 'gameplay': 3})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '=', 'gameplay']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.update(tokenizer(next(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('=', 4), ('gameplay', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=4096)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    ">>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    ">>> src = torch.rand(10, 32, 512)\n",
    ">>> out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf],\n",
       "        [0., 0., -inf],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.masked_fill_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
