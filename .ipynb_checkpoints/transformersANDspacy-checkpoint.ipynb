{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical workflows  \n",
    "\n",
    "initiate a model with model_type and model_name, e.g.,  \n",
    "'distilbert', 'distilbert-base-uncased-distilled-squad' in https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack  \n",
    "'roberta', 'roberta-large' in https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing  \n",
    "respectivly\n",
    "\n",
    "check model_doc for the selected model, especially for    \n",
    "overview  \n",
    "tokenizer to use  \n",
    "class transformers.{model_type}Model, outputting raw hidden-states without any specific head on top  \n",
    "on which forward method about inputs required\n",
    "\n",
    "EXAMPLES  \n",
    "for roberta, check https://huggingface.co/transformers/model_doc/roberta.html  \n",
    "we may know roberta use Byte-level BPE, token_type_ids not required, instead separate segments with \\</s>  \n",
    "for BERT, https://huggingface.co/transformers/model_doc/bert.html  \n",
    "based on WordPiece, check forward about inputs required\n",
    "\n",
    "tokenizers generates inputs for models, forward pass,  \n",
    "but we need to generates targets for loss, backward propagation  \n",
    "in some cases, we also need to generate supplement information for scoring\n",
    "\n",
    "EXAMPLE  \n",
    "check https://www.kaggle.com/sblroid/tweet-sentiment-extraction-fromscratch/\n",
    "\n",
    "More on  \n",
    "*https://huggingface.co/transformers/training.html  \n",
    "https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation transformers package\n",
    "\n",
    "about common methods of classes are found in MAIN CLASSES tab on the left\n",
    "\n",
    "for example, about tokenizers are in https://huggingface.co/transformers/main_classes/tokenizer.html  \n",
    "\\__call__ method is a general method used in a number of tokenizers  \n",
    "\n",
    "but, since tokenizers are inherited from PreTrainedTokenizerBase,  \n",
    "where some common methods such as tokenizer.from_pretrained() are defined,  \n",
    "we may also need to refer INTERNAL HELPERS/Utilities for Tokenizers on the left  \n",
    "https://huggingface.co/transformers/internal/tokenization_utils.html#pretrainedtokenizerbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheetSheat\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "encoding = tokenizer(df_train.text[0:5].tolist(), df_train.selected_text[0:5].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers Theory Summary  \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html\n",
    "\n",
    "naive tokenizations generate a too big vocabulary so subword tokenization is used instaed\n",
    "\n",
    "Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model \n",
    "only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.\n",
    "\n",
    "Three main types: BPE, WordPiece, SentencePiece\n",
    "\n",
    "BPE  \n",
    "first perform naive pre-tokenization  \n",
    "create a base vocabulary of all symbols and learns merge rules  \n",
    ": 'hug' -> 'h' 'u' 'g' -> 'h' 'ug'  \n",
    "does to until the desired size\n",
    "\n",
    "Byte-level BPE  \n",
    "for roberta\n",
    "\n",
    "WordPiece  \n",
    "WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra.  \n",
    "very similar to BPE  \n",
    "WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to make ensure it’s worth it.\n",
    "\n",
    "SentencePiece  \n",
    "All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words.  \n",
    "All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.TokernizerFast in use \n",
    "\n",
    "tokenizer(text, [text], ... ,return_offsets_mapping=True)  \n",
    "return_offsets_mapping=True only available on fast tokenizerz inheriting from PreTrainedTokenizerFast\n",
    "\n",
    "NOTE THAT PreTrainedTokenizerFast is the base class for all fast tokenizers wrapping HuggingFace tokenizers library\n",
    "\n",
    "the required files for offline tokenizers package can be obtained by  \n",
    "transformers.BertTokenizerFast.save_pretrained(PATH_TO_DIR)  \n",
    "which generates vocab.txt, merges.txt, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizers in use  \n",
    "\n",
    "encoding = tokenizer.encode(STR)  \n",
    "encoding.ids  \n",
    "encoding.offsets  \n",
    "\n",
    "no need to specify keyword for offsets  \n",
    "simpler to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about input_ids, offsets generating from tokenizer\n",
    "\n",
    "tokens and inputs_ids, 1-1 correspondence  \n",
    "EXAMPLE  \n",
    "I am a student -> [101, 1045, 2572, 1037, 3076, 102], by BertWordPieceTokenization  \n",
    "each element is the dictionary key for the token  \n",
    "101 and 102 are special tokens\n",
    "\n",
    "input_ids and offsets, 1-1 correspondence  \n",
    "offsets[i] = (char_start, char_end) for the token of input_ids[i]  \n",
    "original_text[char_start: char_end] corresponds to a token since indices in a offset are with respect to the original text string,  \n",
    "EXAMPLE  \n",
    "i am a student -> [(0, 0), (0, 1), (2, 4), (5, 6), (7, 14), (0, 0)]  \n",
    "offsets[2] = (2, 4)  \n",
    "input_ids[2] = 2572  \n",
    "'i am a student'[2: 4] = 'am'\n",
    "\n",
    "by looking offset, we can get the information of how tokens were separated, without encoding selected_text  \n",
    "without offset, for locating selected_text in input_ids, we should identify ids-token matches  \n",
    "EXAMPLE  \n",
    "[4997, 102, 17111, 2080, 6517, 1045, 2097] input_ids of tweet, [6517, 1045] ids of selected_text  \n",
    "but without offsets, we should encode selected_text and compare ids to locate selected_text in input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful CHEATSHEET  \n",
    "https://www.kaggle.com/debanga/huggingface-tokenizers-cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIPS  \n",
    "\n",
    "tokenizer receives  \n",
    "a STR  \n",
    "or a list of STR  \n",
    "or a pair of lists of STR  \n",
    "\n",
    "EXAMPLE  \n",
    "encoding1 = tokenizer(df_train.text[0:5].tolist(), df_train.selected_text[0:5].tolist(), return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.Tokenizer module vs tokenizers package\n",
    "\n",
    "transformers.BertTokenizer.from_pretrained('bert-base-uncased' or path)  \n",
    "many other pretrained models are listed in  \n",
    "https://huggingface.co/transformers/_modules/transformers/tokenization_bert.html\n",
    "\n",
    "tokenizers.BertWordPieceTokenizer(\n",
    "    f\"{BERT_PATH}/vocab.txt\", \n",
    "    lowercase=True\n",
    ")  \n",
    "vocab.txt can be obtained by  \n",
    "https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained\n",
    "\n",
    "Since tweet-sentiment-extraction notebooks cannot access to the Internet,  \n",
    "kernels used tokenizers package with save_pretrained  \n",
    "\n",
    "We may use transformers.TokenizerFast instead, but it is a wrapper for the tokenizers package, so may be slower  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers.roberta  \n",
    "https://huggingface.co/transformers/model_doc/roberta.html  \n",
    "\n",
    "RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n",
    "\n",
    "RoBERTa doesn’t have token_type_ids, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or </s>)\n",
    "\n",
    "Checking for the architecture  \n",
    "roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH)  \n",
    "roberta\n",
    "shows that \n",
    "embedding  \n",
    "encoder comprised of 12 RobertaLayer  \n",
    "pooler of Dense with tanh activation\n",
    "\n",
    "INPUT for roberta  \n",
    "forward(self, input_ids, attention_mask, token_type_ids, ...)  \n",
    "input_ids:  \n",
    "Indices of input sequence tokens in the vocabulary.  \n",
    "attention_mask:  \n",
    "Mask to avoid performing attention on padding token indices.  \n",
    "https://huggingface.co/transformers/glossary.html#attention-mask  \n",
    "This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.  \n",
    "token_type_ids:  \n",
    "https://huggingface.co/transformers/glossary.html#token-type-ids  \n",
    "Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:  \n",
    "The first sequence, the “context” used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers vs simpletransformers  \n",
    "simpletransformers is the package built on transformers for easy use  \n",
    "model_type, model_name arguments are identical  \n",
    "?the form required for train data is also identical  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual steps for using simpletransformrs  \n",
    "Preparing train data as json of the format\n",
    "https://simpletransformers.ai/docs/qa-data-formats/\n",
    "\n",
    "Initiate a model with model_type and model_name  \n",
    "model_type : bert, roberta, distilled, etc.  \n",
    "model_name: specifies the exact architecture and trained weights to use  \n",
    "https://simpletransformers.ai/docs/usage/#task-specific-models  \n",
    "https://simpletransformers.ai/docs/qa-model/\n",
    "\n",
    "model.train_model(PATH_TO_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOW to use SPACY  \n",
    "codes similar here  \n",
    "https://yujuwon.tistory.com/entry/spaCy-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-Training-Models\n",
    "\n",
    "https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718\n",
    "    \n",
    "https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-ensemble?scriptVersionId=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training data for entity recognition  \n",
    "(text, {'entities': [(start, end, class)]})  \n",
    "text[start:end] -> entities in text  \n",
    "\n",
    "Example  \n",
    "('I like London and Berlin.', { 'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The belows are from https://www.kaggle.com/sblroid/tweet-sentiment-extraction-eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(sentiment):\n",
    "    '''\n",
    "    Returns Trainong data in the format needed to train spacy NER\n",
    "    '''\n",
    "    train_data = []\n",
    "    for index, row in df_train.iterrows():\n",
    "        if row.sentiment == sentiment:\n",
    "            selected_text = row.selected_text\n",
    "            text = row.text\n",
    "            start = text.find(selected_text)\n",
    "            end = start + len(selected_text)\n",
    "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
    "            # (text, selected_text)\n",
    "            # selected_text = text[start:end]\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_path(sentiment):\n",
    "    if sentiment == 'positive':\n",
    "        model_out_path = 'models/model_pos'\n",
    "    elif sentiment == 'negative':\n",
    "        model_out_path = 'models/model_neg'\n",
    "    else:\n",
    "        model_out_path = None\n",
    "    return model_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, output_dir, n_iter=20, model=None):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(output_dir)\n",
    "        print(f\"Loaded model '{model}'\")\n",
    "    else:\n",
    "        nlp = spacy.blank('en')\n",
    "        print(\"Created blank 'en' model\")\n",
    "        \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "    # as a result, a pipe in nlp and we got ner to add labels\n",
    "        \n",
    "    for _, annotations in train_data:  # annotations : Dict\n",
    "        for ent in annotations.get('entities'):  # 'entities': [[start, end, 'selected_text']]\n",
    "            ner.add_label(ent[2])  # ent[2]: 'selected_text'\n",
    "    \n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        else:\n",
    "            nlp.resume_training()\n",
    "            \n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=compounding(4, 500, 1.001))\n",
    "            # minibatch : given train_data, returns a generator yielding batches of the given size\n",
    "            # compounding(4, 500, 1.001):start 4, next 4 * 1.001, end 500\n",
    "            # in this example, size=4 and size=compounding(4, 500, 1.001) generates the same result\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                # texts: a tuple of text of size 4 \n",
    "                # annotations: a tuple of dict of size 4\n",
    "                nlp.update(texts, annotations, drop=0.5, losses=losses)\n",
    "            print('Losses', losses)\n",
    "    save_model(output_dir, nlp, 'st_ner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
