{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from chapter82pytorch, based on Section 8.2 in AlgoTarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from Preprocessing import generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq = X_train.shape[1]\n",
    "n_features = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn(nn.Module):\n",
    "    def __init__(self, n_units, n_features, n_layers=5):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(n_features, n_units, n_layers=n_layers, batch_first=True, dropout=0.25)\n",
    "        self.linear1 = nn.Linear(n_units, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h_1, c_1) = self.lstm1(x)        \n",
    "        logits = self.linear1(x[:, -1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn2BN(nn.Module):\n",
    "    def __init__(self, n_units, n_features):\n",
    "        super(Rnn2BN, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(n_features, n_units, batch_first=True)\n",
    "        # self.bn1 = nn.Batch\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.lstm2 = nn.LSTM(n_units, n_units, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.linear1 = nn.Linear(n_units, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h_1, c_1) = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, (h_2, c_2) = self.lstm2(x, (h_1, c_1))\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.linear1(x[:, -1])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn5BN(nn.Module):\n",
    "    def __init__(self, n_units, n_features, len_seq):\n",
    "        super(Rnn5BN, self).__init__()\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(n_features, n_units, batch_first=True)\n",
    "        # self.bn1 = nn.BatchNorm1d(batch_size)\n",
    "        self.ln1 = nn.LayerNorm([len_seq, n_units])\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(n_units, n_units, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(n_units, n_units, batch_first=True)\n",
    "        # self.bn3 = nn.BatchNorm1d(batch_size)\n",
    "        self.ln3 = nn.LayerNorm([len_seq, n_units])\n",
    "        \n",
    "        self.lstm4 = nn.LSTM(n_units, n_units, batch_first=True)\n",
    "        self.dropout4 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.lstm5 = nn.LSTM(n_units, n_units, batch_first=True)\n",
    "        # self.bn5 = nn.BatchNorm1d(batch_size)\n",
    "        self.ln5 = nn.LayerNorm([len_seq, n_units])\n",
    "        \n",
    "        self.linear1 = nn.Linear(n_units, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (h_1, c_1) = self.lstm1(x)\n",
    "        x = self.ln1(x)\n",
    "        # x = self.bn1(x.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        \n",
    "        x, (h_2, c_2) = self.lstm2(x, (h_1, c_1))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x, (h_3, c_3) = self.lstm3(x, (h_2, c_2))\n",
    "        x = self.ln3(x)\n",
    "        # x = self.bn3(x.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        \n",
    "        x, (h_4, c_4) = self.lstm4(x, (h_3, c_3))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x, (h_5, c_5) = self.lstm5(x, (h_4, c_4))\n",
    "        x = self.ln5(x)\n",
    "        # x = self.bn5(x.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        \n",
    "        logits = self.linear1(x[:, -1])\n",
    "\n",
    "        \n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Rnn(num_unit=200, seq_len=5)\n",
    "# model = Rnn2BN(num_units=100, num_features=22, seq_len=5)\n",
    "model = Rnn5BN(n_units=200, n_features=n_features, len_seq=len_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 2\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = torch.from_numpy(X_train).float()\n",
    "y_train_ = torch.from_numpy(y_train).long()\n",
    "X_test_ = torch.from_numpy(X_test).float()\n",
    "y_test_ = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_, y_train_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_loop(epochs, tensorboard=False):\n",
    "    # model.train()\n",
    "    size = len(train_dataloader.dataset)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        train_loss = 0.\n",
    "        train_score = 0.\n",
    "        if tensorboard:\n",
    "            writer = SummaryWriter(f\"runs/AlgoTradeEpoch{e}\")\n",
    "        \n",
    "        for i, (X, y) in enumerate(train_dataloader, 1):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "                        \n",
    "            if tensorboard:\n",
    "                for n, p in model.named_parameters():\n",
    "                    if 'weight' in n:\n",
    "                        writer.add_histogram(f\"{n}\", p.grad, i)\n",
    "                        writer.add_scalar(f\"{n}_abs_mean\", p.grad.abs().mean(), i)\n",
    "                        # the name of histogram and scalar should be different, if not, both of them cannot be recognized\n",
    "                writer.close()\n",
    "                \n",
    "            \n",
    "        if e % 10 == 0:\n",
    "            model.eval()\n",
    "            pred = model(X_test_)\n",
    "            loss = loss_fn(pred, y_test_)\n",
    "            print(f\"|TRAIN| Epoch: {e:3d}, Loss: {train_loss/i:.6f}   |TEST| Epoch: {e:3d}, Loss: {loss.item():.6f}\")\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|TRAIN| Epoch:   0, Loss: 0.653356   |TEST| Epoch:   0, Loss: 0.711697\n",
      "|TRAIN| Epoch:  10, Loss: 0.652863   |TEST| Epoch:  10, Loss: 0.712334\n",
      "|TRAIN| Epoch:  20, Loss: 0.656736   |TEST| Epoch:  20, Loss: 0.712126\n",
      "|TRAIN| Epoch:  30, Loss: 0.653080   |TEST| Epoch:  30, Loss: 0.714316\n",
      "|TRAIN| Epoch:  40, Loss: 0.651470   |TEST| Epoch:  40, Loss: 0.713698\n",
      "|TRAIN| Epoch:  50, Loss: 0.651356   |TEST| Epoch:  50, Loss: 0.714710\n",
      "|TRAIN| Epoch:  60, Loss: 0.648487   |TEST| Epoch:  60, Loss: 0.715051\n",
      "|TRAIN| Epoch:  70, Loss: 0.650429   |TEST| Epoch:  70, Loss: 0.714781\n",
      "|TRAIN| Epoch:  80, Loss: 0.644368   |TEST| Epoch:  80, Loss: 0.714947\n",
      "|TRAIN| Epoch:  90, Loss: 0.646378   |TEST| Epoch:  90, Loss: 0.717084\n"
     ]
    }
   ],
   "source": [
    "train_loop(epochs=100, tensorboard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__jit_unused_properties__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_weights',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_flat_weights',\n",
       " '_flat_weights_names',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'all_weights',\n",
       " 'apply',\n",
       " 'batch_first',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'bias_hh_l0',\n",
       " 'bias_ih_l0',\n",
       " 'bidirectional',\n",
       " 'buffers',\n",
       " 'check_forward_args',\n",
       " 'check_hidden_size',\n",
       " 'check_input',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'flatten_parameters',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_expected_cell_size',\n",
       " 'get_expected_hidden_size',\n",
       " 'half',\n",
       " 'hidden_size',\n",
       " 'input_size',\n",
       " 'load_state_dict',\n",
       " 'mode',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_layers',\n",
       " 'parameters',\n",
       " 'permute_hidden',\n",
       " 'proj_size',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight_hh_l0',\n",
       " 'weight_ih_l0',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model.lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [l.numel() for l in model.lstm1.all_weights[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm1.bias_hh_l0.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm1.bias_ih_l0.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179200"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm1.weight_hh_l0.numel() + model.lstm1.weight_ih_l0.numel() + 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    writer.add_scalar(f\"testing_abs_mean\", i+10, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = net(torch.randn(10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(preds, torch.randint(0,5, (10,)).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_input_dim',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'affine',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'bias',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eps',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'momentum',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_batches_tracked',\n",
       " 'num_features',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'reset_running_stats',\n",
       " 'running_mean',\n",
       " 'running_var',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'track_running_stats',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
