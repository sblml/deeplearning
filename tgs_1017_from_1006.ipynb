{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size 32, horizontal crop, different scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different learning rate scheduling\n",
    "# apply lr scheudle in this notebook to other models\n",
    "# cosine annealing from the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BORDER_REPLICATE\n",
    "# no center upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.io import imread\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from skimage.exposure import adjust_gamma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torchvision import models, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "PATH = '../input/'\n",
    "if os.path.exists('../input/tgs-salt-identification-challenge/'):\n",
    "    PATH = '../input/tgs-salt-identification-challenge/'\n",
    "TR_IMG_PATH = PATH + 'train/images/'\n",
    "TR_MASK_PATH = PATH + 'train/masks/'\n",
    "TE_IMG_PATH = PATH + 'test/images/'\n",
    "\n",
    "SEED = 2018\n",
    "N_SPLITS = 4\n",
    "img_size = 101\n",
    "\n",
    "VERSION = 'tgs_1016'\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "CHECKPOINT = False\n",
    "SUBMIT = False\n",
    "\n",
    "# LOG = 'tgs_1004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4874b7e5a7ed4331b609d17e25194f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527e134d23334382a413d56b163f59b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(PATH + 'train.csv', index_col='id')\n",
    "depths = pd.read_csv(PATH + 'depths.csv', index_col='id')\n",
    "df_train['depth'] = depths\n",
    "df_test = pd.DataFrame(index=depths.index.drop(df_train.index))\n",
    "df_test['depth'] = depths\n",
    "\n",
    "def preprocess_img_gray(f):\n",
    "    return (imread(f)/65535)[..., np.newaxis]\n",
    "def preprocess_img_rgb(f):\n",
    "    return imread(f)/255\n",
    "\n",
    "images_train = np.empty((df_train.shape[0], img_size, img_size, 3), dtype=np.float32)\n",
    "masks_train = np.empty((df_train.shape[0], img_size, img_size, 1), dtype=np.float32)\n",
    "images_test = np.empty((df_test.shape[0], img_size, img_size, 3), dtype=np.float32)\n",
    "for i, f in enumerate(tqdm_notebook(df_train.index)):\n",
    "    images_train[i] = preprocess_img_rgb(TR_IMG_PATH + f + '.png')\n",
    "    masks_train[i] = preprocess_img_gray(TR_MASK_PATH + f + '.png')\n",
    "for i, f in enumerate(tqdm_notebook(df_test.index)):\n",
    "    images_test[i] = preprocess_img_rgb(TE_IMG_PATH + f + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import model_zoo, model_urls, BasicBlock\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        # self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def resnet34(pretrained=False):\n",
    "    model = ResNet34(BasicBlock, [3, 4, 6, 3])\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBnRelu2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "\n",
    "class cSE(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels//2)\n",
    "        self.fc2 = nn.Linear(channels//2, channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.avg(x).squeeze()\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = torch.sigmoid(self.fc2(z))\n",
    "        return z.reshape(*z.shape, 1, 1) * x\n",
    "\n",
    "    \n",
    "class scSE(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels//reduction)\n",
    "        self.fc2 = nn.Linear(channels//reduction, channels)\n",
    "        self.conv = nn.Conv2d(channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.avg(x).squeeze()\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = torch.sigmoid(self.fc2(z))\n",
    "        cse = z.reshape(*z.shape, 1, 1) * x\n",
    "        \n",
    "        q = torch.sigmoid(self.conv(x))\n",
    "        sse = q * x\n",
    "        \n",
    "        return cse + sse\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, mid_channels, out_channels,\n",
    "                 kernel_size=3, padding=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = ConvBnRelu2d(in_channels, mid_channels,\n",
    "                                  kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = ConvBnRelu2d(mid_channels, out_channels,\n",
    "                                  kernel_size=3, stride=1, padding=1)\n",
    "        self.scse = scSE(out_channels)\n",
    "    \n",
    "    def forward(self, x, e=None, upsample=True):\n",
    "        if upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        if e is not None:\n",
    "            x = torch.cat([x, e], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.scse(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SEResNet34Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            self.resnet.conv1,\n",
    "            self.resnet.bn1,\n",
    "            self.resnet.relu\n",
    "        )\n",
    "        self.encoder1 = self.resnet.layer1  # 64\n",
    "        self.encoder2 = self.resnet.layer2  # 128\n",
    "        self.encoder3 = self.resnet.layer3  # 256\n",
    "        self.encoder4 = self.resnet.layer4  # 512\n",
    "        \n",
    "        self.scse1 = scSE(64)\n",
    "        self.scse2 = scSE(128)\n",
    "        self.scse3 = scSE(256)\n",
    "        self.scse4 = scSE(512)\n",
    "        \n",
    "        self.center = nn.Sequential(\n",
    "            ConvBnRelu2d(512, 512, kernel_size=3, padding=1),\n",
    "            ConvBnRelu2d(512, 256, kernel_size=3, padding=1),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),  # this step enables to add decoder0\n",
    "        )\n",
    "        \n",
    "        self.decoder4 = Decoder(512 + 256, 512, 64)  # bottleneck\n",
    "        self.decoder3 = Decoder(256 +  64, 256, 64)\n",
    "        self.decoder2 = Decoder(128 +  64, 128, 64)\n",
    "        self.decoder1 = Decoder( 64 +  64,  64, 64)\n",
    "        self.decoder0 = Decoder( 64,        32, 64)\n",
    "        \n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)  # 64\n",
    "        \n",
    "        e1 = self.encoder1(x)  # 64\n",
    "        e1 = self.scse1(e1)\n",
    "        e2 = self.encoder2(e1)  # 32\n",
    "        e2 = self.scse2(e2)\n",
    "        e3 = self.encoder3(e2)  # 16\n",
    "        e3 = self.scse3(e3)\n",
    "        e4 = self.encoder4(e3)  # 8\n",
    "        e4 = self.scse4(e4)\n",
    "        \n",
    "        c = self.center(e4)  # 4\n",
    "        \n",
    "        d4 = self.decoder4( c, e4, upsample=False)\n",
    "        d3 = self.decoder3(d4, e3)\n",
    "        d2 = self.decoder2(d3, e2)\n",
    "        d1 = self.decoder1(d2, e1)\n",
    "        d0 = self.decoder0(d1)\n",
    "        \n",
    "        h = torch.cat([\n",
    "            d0,\n",
    "            F.interpolate(d1, scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d2, scale_factor=4, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d3, scale_factor=8, mode='bilinear', align_corners=False),\n",
    "            F.interpolate(d4, scale_factor=16, mode='bilinear', align_corners=False),\n",
    "        ], dim=1)\n",
    "        # h = F.dropout2d(h, p=0.2)\n",
    "        h = self.logit(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGSdataset(Dataset):\n",
    "    def __init__(self, X, y, augment):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        mask = self.y[idx]\n",
    "        return self.augment(image, mask)\n",
    "    \n",
    "def train_augmentation(Xi, yi):\n",
    "    if np.random.uniform() > 0.5:\n",
    "        Xi = Xi[:, ::-1, :]\n",
    "        yi = yi[:, ::-1, :]\n",
    "    \n",
    "    if np.random.uniform() > 0.5:\n",
    "        sw = np.random.randint(3)\n",
    "        if sw == 0:\n",
    "            xl, xr, yu, yd = np.random.randint(0, 11, 4)\n",
    "            Xi = Xi[:, xl:101-xr, :]\n",
    "            yi = yi[:, xl:101-xr, :]            \n",
    "#             Xi = Xi[yu:101-yd, xl:101-xr, :]\n",
    "#             yi = yi[yu:101-yd, xl:101-xr, :]\n",
    "            Xi = cv2.resize(Xi, dsize=(101, 101))\n",
    "            yi = cv2.resize(yi, dsize=(101, 101), interpolation=cv2.INTER_NEAREST) \n",
    "            yi = (yi > 0.5).astype(np.float32)\n",
    "        if sw == 1:\n",
    "            dx = np.random.randint(-10, 11)\n",
    "            M = np.array([[1, -2*dx/101, dx], [0, 1, 0]])  # cot(shr) = 2*dx / 101\n",
    "            Xi = cv2.warpAffine(Xi, M, (101, 101), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "            yi = cv2.warpAffine(yi, M, (101, 101), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REFLECT_101)\n",
    "            yi = (yi > 0.5).astype(np.float32)\n",
    "        if sw == 2:\n",
    "            deg = np.random.uniform(-10, 10)\n",
    "            M = cv2.getRotationMatrix2D((101/2, 101/2), deg, 1)\n",
    "            Xi = cv2.warpAffine(Xi, M, (101, 101), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "            yi = cv2.warpAffine(yi, M, (101, 101), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REFLECT_101)\n",
    "            yi = (yi > 0.5).astype(np.float32)\n",
    "            \n",
    "    if np.random.uniform() > 0.5:\n",
    "        sw = np.random.randint(3)\n",
    "        if sw == 0:\n",
    "            gamma = np.random.uniform(0.95, 1.05)\n",
    "            Xi = np.clip(Xi ** gamma, 0, 1)\n",
    "        if sw == 1:\n",
    "            brs = np.random.uniform(-0.05, 0.05)\n",
    "            Xi = np.clip(Xi + brs, 0, 1)\n",
    "        if sw == 2:\n",
    "            brm = np.random.uniform(0.95, 1.05)\n",
    "            Xi = np.clip(Xi * brm, 0, 1)\n",
    "            \n",
    "    Xi = cv2.copyMakeBorder(Xi, 14, 13, 14, 13, cv2.BORDER_REPLICATE)  # cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT_101\n",
    "    yi = cv2.copyMakeBorder(yi, 14, 13, 14, 13, cv2.BORDER_REPLICATE)[...,np.newaxis]\n",
    "    # Xi = cv2.resize(Xi, dsize=(128, 128), cv2.INTER_LINEAR)\n",
    "    # yi = cv2.resize(yi, dsize=(128, 128), cv2.INTER_NEAREST)[...,np.newaxis]\n",
    "    return Xi, yi\n",
    "\n",
    "def valid_augmentation(Xi, yi):\n",
    "    Xi = cv2.copyMakeBorder(Xi, 14, 13, 14, 13, cv2.BORDER_REPLICATE)\n",
    "    yi = cv2.copyMakeBorder(yi, 14, 13, 14, 13, cv2.BORDER_REPLICATE)[...,np.newaxis]\n",
    "    # Xi = cv2.resize(Xi, dsize=(128, 128), cv2.INTER_LINEAR)\n",
    "    # yi = cv2.resize(yi, dsize=(128, 128), cv2.INTER_NEAREST)[...,np.newaxis]\n",
    "    return Xi, yi\n",
    "\n",
    "class TGSdataset_test(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        Xi = self.X[idx]\n",
    "        return cv2.copyMakeBorder(Xi, 14, 13, 14, 13, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "# got batches of a list of tuples (image, mask)\n",
    "def tgs_collate(batch):\n",
    "    len_batch = len(batch)\n",
    "    image_batch = []\n",
    "    mask_batch = []\n",
    "    for i in range(len_batch):\n",
    "        image_batch.append(batch[i][0])\n",
    "        mask_batch.append(batch[i][1])\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image_batch = torch.from_numpy((np.array(image_batch) - mean) / std).permute([0, 3, 1, 2])\n",
    "    mask_batch = torch.from_numpy(np.array(mask_batch)).permute([0, 3, 1, 2])\n",
    "    return image_batch.float(), mask_batch.float()\n",
    "\n",
    "def tgs_collate_test(batch):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image_batch = torch.from_numpy((np.array(batch) - mean) / std).permute([0, 3, 1, 2])\n",
    "    return image_batch.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = np.ceil(masks_train.sum(axis=(1,2,3)) / 101**2 * 10)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED).split(np.zeros(len(images_train)), coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idx, val_idx = next(skf)\n",
    "images_tr, images_val, masks_tr, masks_val =\\\n",
    "images_train[tr_idx], images_train[val_idx], masks_train[tr_idx], masks_train[val_idx]\n",
    "train_dataset = TGSdataset(images_tr, masks_tr, train_augmentation)\n",
    "valid_dataset = TGSdataset(images_val, masks_val, valid_augmentation)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True, collate_fn=tgs_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True, collate_fn=tgs_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "# customed\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.squeeze(1).unsqueeze(0), lab.squeeze(1).unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits.squeeze(1), labels.squeeze(1), ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu(errors_sorted)+1, Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS ---------------------------\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_score = 0.0\n",
    "    n_batch = len(valid_loader)\n",
    "    for i, (img_batch, mask_batch) in enumerate(valid_loader):\n",
    "        img_batch = img_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = net(img_batch)\n",
    "            loss = criterion(outputs, mask_batch)\n",
    "            score = iou(mask_batch, torch.sigmoid(outputs))\n",
    "        valid_loss += loss.item()\n",
    "        valid_score += score.item()\n",
    "    model.train()\n",
    "    return valid_loss / n_batch, valid_score / n_batch\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    preds = y_pred.round().float()\n",
    "    preds += y_true.float()\n",
    "    u_ = (preds > 0).sum(dim=(2,3), dtype=torch.float)\n",
    "    i_ = (preds > 1).sum(dim=(2,3), dtype=torch.float)\n",
    "    iou = (i_ + 1e-5) / (u_ + 1e-5)\n",
    "    threshold = torch.arange(0.5, 1.0, 0.05).cuda()\n",
    "    return (iou > threshold).float().mean()\n",
    "\n",
    "# def iou(y_true, y_pred, thr=0.5):\n",
    "#     preds = (y_pred > thr).float()\n",
    "#     preds += y_true\n",
    "#     u_ = (preds > 0).sum(dim=(2,3), dtype=torch.float)  # do not squeeze to apply threshold\n",
    "#     i_ = (preds == 2).sum(dim=(2,3), dtype=torch.float)\n",
    "#     iou = torch.where(u_ == 0, torch.ones_like(u_), i_/u_)\n",
    "#     threshold = torch.arange(0.5, 1.0, 0.05).cuda()\n",
    "#     return (iou > threshold).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch=1, scheduler=None, early_stopping=False, best_checkpoint=True, verbose=1):\n",
    "    model.train()\n",
    "    iter_valid = 300\n",
    "    n_batch = len(train_loader)\n",
    "\n",
    "    best_validation_score = 0.0\n",
    "    best_train_score = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    log_fn = f'log/{VERSION}_{start_epoch}_{start_epoch+epochs-1}.log'\n",
    "    if os.path.exists(log_fn):\n",
    "        for i in range(1, 10):\n",
    "            log_fn = f'log/{VERSION}_{start_epoch}_{start_epoch+epochs-1}_{i}.log'\n",
    "            if not os.path.exists(log_fn):\n",
    "                break\n",
    "    log_file = open(log_fn, 'w')\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        epoch = start_epoch + e\n",
    "        t_ = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_score = 0.0\n",
    "        n_valid = 0\n",
    "        \n",
    "        for i, (img_batch, mask_batch) in enumerate(train_loader, 1):\n",
    "            img_batch = img_batch.float().cuda()\n",
    "            mask_batch = mask_batch.float().cuda()\n",
    "            outputs = net(img_batch) # nn.parallel.data_parallel(model, img_batch)\n",
    "            loss = criterion(outputs, mask_batch)\n",
    "            # loss2 = lovasz_hinge(outputs, mask_batch)\n",
    "            # loss = loss1 + loss2\n",
    "            score = iou(mask_batch, torch.sigmoid(outputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_score += score.item()\n",
    "            \n",
    "            if i % iter_valid == 0:\n",
    "                n_valid += 1\n",
    "                v_l, v_s = validation()\n",
    "                valid_loss += v_l\n",
    "                valid_score += v_s\n",
    "                print('%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f  Valid Loss: %.6f  Valid Score: %.6f\\r'\n",
    "                      % (i, n_batch, epoch, running_loss/i, running_score/i, valid_loss/n_valid, valid_score/n_valid), end= \"\")\n",
    "                continue\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print('%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f\\r'\n",
    "                      % (i, n_batch, epoch, running_loss/i, running_score/i), end= \"\")\n",
    "        # end for in enumerate(train_loader, 1)\n",
    "\n",
    "        n_valid += 1\n",
    "        v_l, v_s = validation()\n",
    "        valid_loss += v_l\n",
    "        valid_score += v_s\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(valid_score/n_valid)\n",
    "        \n",
    "        # checkpoint utilities\n",
    "        if valid_score/n_valid > best_validation_score:\n",
    "            best_validation_score = valid_score / n_valid\n",
    "            best_train_score = running_score / i\n",
    "            best_epoch = epoch\n",
    "            if best_checkpoint:\n",
    "                model_state_dict = model.state_dict()\n",
    "                optimizer_state_dict = optimizer.state_dict()\n",
    "        if early_stopping:\n",
    "            if epoch - best_epoch == early_stopping:\n",
    "                break\n",
    "\n",
    "        log_message = '%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f  Valid Loss: %.6f  Valid Score: %.6f  %3d Sec.'\\\n",
    "        % (i, n_batch, epoch, running_loss/i, running_score/i, valid_loss/n_valid, valid_score/n_valid, time.time() - t_)\n",
    "        print(log_message)\n",
    "        print(log_message, file=log_file)\n",
    "\n",
    "    # end for in range(epochs)\n",
    "    \n",
    "    log_file.close()\n",
    "    # save model\n",
    "    if best_checkpoint:\n",
    "        f_ = f'{VERSION}_{best_epoch}_{best_validation_score:.5f}.pt'\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'best_validation_score': best_validation_score,\n",
    "            'best_train_score': best_train_score,\n",
    "        }, f_)\n",
    "\n",
    "    f_ = f'{VERSION}_{epoch}_{valid_score/n_valid:.5f}.pt'\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_score': running_score/n_batch,\n",
    "        'validation_score': valid_score,\n",
    "    }, f_)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEResNet34Unet()\n",
    "net = model.cuda()\n",
    "# net = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lovasz_hinge\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 / 112, Epoch:   1  Train Loss: 2.008440  Train Score: 0.277344  Valid Loss: 1.996181  Valid Score: 0.450000   33 Sec.\n",
      "112 / 112, Epoch:   2  Train Loss: 1.824871  Train Score: 0.538114  Valid Loss: 1.536339  Valid Score: 0.632813   33 Sec.\n",
      "112 / 112, Epoch:   3  Train Loss: 1.454331  Train Score: 0.653125  Valid Loss: 1.362920  Valid Score: 0.672656   33 Sec.\n",
      "112 / 112, Epoch:   4  Train Loss: 1.370753  Train Score: 0.674749  Valid Loss: 1.326932  Valid Score: 0.685417   33 Sec.\n",
      "112 / 112, Epoch:   5  Train Loss: 1.194225  Train Score: 0.719978  Valid Loss: 1.211974  Valid Score: 0.715885   34 Sec.\n",
      "112 / 112, Epoch:   6  Train Loss: 1.054763  Train Score: 0.759263  Valid Loss: 1.041610  Valid Score: 0.765625   34 Sec.\n",
      "112 / 112, Epoch:   7  Train Loss: 1.041133  Train Score: 0.756222  Valid Loss: 0.996129  Valid Score: 0.767448   34 Sec.\n",
      "112 / 112, Epoch:   8  Train Loss: 0.992856  Train Score: 0.766964  Valid Loss: 0.948855  Valid Score: 0.769271   34 Sec.\n",
      "112 / 112, Epoch:   9  Train Loss: 0.919285  Train Score: 0.783315  Valid Loss: 0.897881  Valid Score: 0.789583   34 Sec.\n",
      "112 / 112, Epoch:  10  Train Loss: 0.872820  Train Score: 0.794336  Valid Loss: 0.914398  Valid Score: 0.787500   33 Sec.\n",
      "112 / 112, Epoch:  11  Train Loss: 0.872743  Train Score: 0.798633  Valid Loss: 0.890210  Valid Score: 0.793229   34 Sec.\n",
      "112 / 112, Epoch:  12  Train Loss: 0.866734  Train Score: 0.800223  Valid Loss: 0.960425  Valid Score: 0.778646   33 Sec.\n",
      "112 / 112, Epoch:  13  Train Loss: 0.822643  Train Score: 0.809821  Valid Loss: 0.883314  Valid Score: 0.792708   34 Sec.\n",
      "112 / 112, Epoch:  14  Train Loss: 0.796400  Train Score: 0.816323  Valid Loss: 0.828859  Valid Score: 0.810938   34 Sec.\n",
      "112 / 112, Epoch:  15  Train Loss: 0.776348  Train Score: 0.818276  Valid Loss: 0.817872  Valid Score: 0.802865   34 Sec.\n",
      "112 / 112, Epoch:  16  Train Loss: 0.769008  Train Score: 0.820089  Valid Loss: 0.820737  Valid Score: 0.812500   33 Sec.\n",
      "112 / 112, Epoch:  17  Train Loss: 0.751856  Train Score: 0.820285  Valid Loss: 0.850067  Valid Score: 0.811198   34 Sec.\n",
      "112 / 112, Epoch:  18  Train Loss: 0.717834  Train Score: 0.832394  Valid Loss: 0.902996  Valid Score: 0.782031   34 Sec.\n",
      "112 / 112, Epoch:  19  Train Loss: 0.693539  Train Score: 0.834598  Valid Loss: 0.859659  Valid Score: 0.777344   34 Sec.\n",
      "112 / 112, Epoch:  20  Train Loss: 0.684054  Train Score: 0.838170  Valid Loss: 0.834309  Valid Score: 0.808594   34 Sec.\n",
      "112 / 112, Epoch:  21  Train Loss: 0.683913  Train Score: 0.836830  Valid Loss: 0.730399  Valid Score: 0.820573   33 Sec.\n",
      "112 / 112, Epoch:  22  Train Loss: 0.676336  Train Score: 0.837946  Valid Loss: 0.801212  Valid Score: 0.813281   33 Sec.\n",
      "112 / 112, Epoch:  23  Train Loss: 0.651340  Train Score: 0.845592  Valid Loss: 0.720900  Valid Score: 0.829167   34 Sec.\n",
      "112 / 112, Epoch:  24  Train Loss: 0.623459  Train Score: 0.849107  Valid Loss: 0.724904  Valid Score: 0.834635   34 Sec.\n",
      "112 / 112, Epoch:  25  Train Loss: 0.628366  Train Score: 0.848382  Valid Loss: 0.823102  Valid Score: 0.810417   34 Sec.\n",
      "112 / 112, Epoch:  26  Train Loss: 0.629849  Train Score: 0.846680  Valid Loss: 0.727535  Valid Score: 0.823177   34 Sec.\n",
      "112 / 112, Epoch:  27  Train Loss: 0.601073  Train Score: 0.855469  Valid Loss: 0.756496  Valid Score: 0.818490   33 Sec.\n",
      "112 / 112, Epoch:  28  Train Loss: 0.588557  Train Score: 0.858705  Valid Loss: 0.727248  Valid Score: 0.822135   33 Sec.\n",
      "112 / 112, Epoch:  29  Train Loss: 0.568817  Train Score: 0.859459  Valid Loss: 0.780336  Valid Score: 0.813542   33 Sec.\n",
      "112 / 112, Epoch:  30  Train Loss: 0.612366  Train Score: 0.850921  Valid Loss: 0.849957  Valid Score: 0.790625   34 Sec.\n"
     ]
    }
   ],
   "source": [
    "VERSION = 'tgs_1017_from_1006'\n",
    "train(epochs=30, best_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.005\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 / 112, Epoch:  31  Train Loss: 0.546963  Train Score: 0.862528  Valid Loss: 0.714388  Valid Score: 0.822656   33 Sec.\n",
      "112 / 112, Epoch:  32  Train Loss: 0.501392  Train Score: 0.875167  Valid Loss: 0.733914  Valid Score: 0.831510   34 Sec.\n",
      "112 / 112, Epoch:  33  Train Loss: 0.462539  Train Score: 0.882031  Valid Loss: 0.762850  Valid Score: 0.823177   34 Sec.\n",
      "112 / 112, Epoch:  34  Train Loss: 0.471827  Train Score: 0.879353  Valid Loss: 0.691209  Valid Score: 0.826302   34 Sec.\n",
      "112 / 112, Epoch:  35  Train Loss: 0.459998  Train Score: 0.883566  Valid Loss: 0.712707  Valid Score: 0.822917   34 Sec.\n",
      "112 / 112, Epoch:  36  Train Loss: 0.436479  Train Score: 0.886998  Valid Loss: 0.732585  Valid Score: 0.820052   33 Sec.\n",
      "112 / 112, Epoch:  37  Train Loss: 0.446938  Train Score: 0.885575  Valid Loss: 0.716011  Valid Score: 0.828125   34 Sec.\n",
      "112 / 112, Epoch:  38  Train Loss: 0.429595  Train Score: 0.889425  Valid Loss: 0.755072  Valid Score: 0.834896   34 Sec.\n",
      "112 / 112, Epoch:  39  Train Loss: 0.425041  Train Score: 0.890206  Valid Loss: 0.807443  Valid Score: 0.825000   34 Sec.\n",
      "112 / 112, Epoch:  40  Train Loss: 0.412511  Train Score: 0.891853  Valid Loss: 0.773752  Valid Score: 0.829427   34 Sec.\n",
      "112 / 112, Epoch:  41  Train Loss: 0.409981  Train Score: 0.894029  Valid Loss: 0.771659  Valid Score: 0.834115   34 Sec.\n",
      "112 / 112, Epoch:  42  Train Loss: 0.406592  Train Score: 0.893415  Valid Loss: 0.753819  Valid Score: 0.834635   33 Sec.\n",
      "112 / 112, Epoch:  43  Train Loss: 0.401845  Train Score: 0.894308  Valid Loss: 0.817912  Valid Score: 0.820573   34 Sec.\n",
      "112 / 112, Epoch:  44  Train Loss: 0.393546  Train Score: 0.895508  Valid Loss: 0.766483  Valid Score: 0.825260   34 Sec.\n",
      "112 / 112, Epoch:  45  Train Loss: 0.412758  Train Score: 0.892411  Valid Loss: 0.699159  Valid Score: 0.823958   34 Sec.\n",
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-03.92885\n",
      "112 / 112, Epoch:  46  Train Loss: 0.399003  Train Score: 0.892885  Valid Loss: 0.747043  Valid Score: 0.820052   34 Sec.\n",
      "112 / 112, Epoch:  47  Train Loss: 0.353300  Train Score: 0.903990  Valid Loss: 0.742419  Valid Score: 0.833333   34 Sec.\n",
      "112 / 112, Epoch:  48  Train Loss: 0.330246  Train Score: 0.910073  Valid Loss: 0.745250  Valid Score: 0.839323   34 Sec.\n",
      "112 / 112, Epoch:  49  Train Loss: 0.325152  Train Score: 0.911663  Valid Loss: 0.749802  Valid Score: 0.830990   34 Sec.\n",
      "112 / 112, Epoch:  50  Train Loss: 0.329599  Train Score: 0.908092  Valid Loss: 0.743592  Valid Score: 0.839844   34 Sec.\n",
      "112 / 112, Epoch:  51  Train Loss: 0.323129  Train Score: 0.912444  Valid Loss: 0.751491  Valid Score: 0.837240   34 Sec.\n",
      "112 / 112, Epoch:  52  Train Loss: 0.311752  Train Score: 0.913170  Valid Loss: 0.707509  Valid Score: 0.839323   34 Sec.\n",
      "112 / 112, Epoch:  53  Train Loss: 0.308068  Train Score: 0.915123  Valid Loss: 0.793419  Valid Score: 0.832292   34 Sec.\n",
      "112 / 112, Epoch:  54  Train Loss: 0.319165  Train Score: 0.913253  Valid Loss: 0.812660  Valid Score: 0.835417   34 Sec.\n",
      "112 / 112, Epoch:  55  Train Loss: 0.300193  Train Score: 0.918108  Valid Loss: 0.739993  Valid Score: 0.844531   34 Sec.\n",
      "112 / 112, Epoch:  56  Train Loss: 0.298448  Train Score: 0.919838  Valid Loss: 0.736533  Valid Score: 0.838802   34 Sec.\n",
      "112 / 112, Epoch:  57  Train Loss: 0.306027  Train Score: 0.917020  Valid Loss: 0.698972  Valid Score: 0.848958   34 Sec.\n",
      "112 / 112, Epoch:  58  Train Loss: 0.310331  Train Score: 0.916099  Valid Loss: 0.778064  Valid Score: 0.840885   34 Sec.\n",
      "112 / 112, Epoch:  59  Train Loss: 0.291016  Train Score: 0.919699  Valid Loss: 0.792467  Valid Score: 0.833333   34 Sec.\n",
      "112 / 112, Epoch:  60  Train Loss: 0.298425  Train Score: 0.920257  Valid Loss: 0.760841  Valid Score: 0.826563   34 Sec.\n",
      "112 / 112, Epoch:  61  Train Loss: 0.286855  Train Score: 0.920871  Valid Loss: 0.797226  Valid Score: 0.827344   34 Sec.\n",
      "112 / 112, Epoch:  62  Train Loss: 0.277249  Train Score: 0.925837  Valid Loss: 0.787507  Valid Score: 0.851042   34 Sec.\n",
      "112 / 112, Epoch:  63  Train Loss: 0.288827  Train Score: 0.921345  Valid Loss: 0.760115  Valid Score: 0.831510   33 Sec.\n",
      "112 / 112, Epoch:  64  Train Loss: 0.297205  Train Score: 0.918890  Valid Loss: 0.774645  Valid Score: 0.835156   33 Sec.\n",
      "112 / 112, Epoch:  65  Train Loss: 0.275803  Train Score: 0.925000  Valid Loss: 0.804584  Valid Score: 0.832031   34 Sec.\n",
      "112 / 112, Epoch:  66  Train Loss: 0.275171  Train Score: 0.926451  Valid Loss: 0.823247  Valid Score: 0.841927   34 Sec.\n",
      "112 / 112, Epoch:  67  Train Loss: 0.275991  Train Score: 0.924888  Valid Loss: 0.802603  Valid Score: 0.834635   34 Sec.\n",
      "112 / 112, Epoch:  68  Train Loss: 0.284370  Train Score: 0.925419  Valid Loss: 0.804360  Valid Score: 0.828906   34 Sec.\n",
      "112 / 112, Epoch:  69  Train Loss: 0.280440  Train Score: 0.925530  Valid Loss: 0.790589  Valid Score: 0.830208   34 Sec.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2500e-03.26730\n",
      "112 / 112, Epoch:  70  Train Loss: 0.278008  Train Score: 0.926730  Valid Loss: 0.814425  Valid Score: 0.839583   34 Sec.\n",
      "112 / 112, Epoch:  71  Train Loss: 0.270783  Train Score: 0.926200  Valid Loss: 0.780034  Valid Score: 0.844531   34 Sec.\n",
      "112 / 112, Epoch:  72  Train Loss: 0.262053  Train Score: 0.931613  Valid Loss: 0.802299  Valid Score: 0.842969   34 Sec.\n",
      "112 / 112, Epoch:  73  Train Loss: 0.259131  Train Score: 0.931613  Valid Loss: 0.787119  Valid Score: 0.840365   34 Sec.\n",
      "112 / 112, Epoch:  74  Train Loss: 0.239348  Train Score: 0.935631  Valid Loss: 0.717797  Valid Score: 0.854167   34 Sec.\n",
      "112 / 112, Epoch:  75  Train Loss: 0.245538  Train Score: 0.934515  Valid Loss: 0.765935  Valid Score: 0.843229   34 Sec.\n",
      "112 / 112, Epoch:  76  Train Loss: 0.250169  Train Score: 0.934152  Valid Loss: 0.825952  Valid Score: 0.839323   34 Sec.\n",
      "112 / 112, Epoch:  77  Train Loss: 0.256164  Train Score: 0.931613  Valid Loss: 0.790366  Valid Score: 0.848958   34 Sec.\n",
      "112 / 112, Epoch:  78  Train Loss: 0.258001  Train Score: 0.933733  Valid Loss: 0.771687  Valid Score: 0.840885   34 Sec.\n",
      "112 / 112, Epoch:  79  Train Loss: 0.243967  Train Score: 0.934542  Valid Loss: 0.827695  Valid Score: 0.837240   34 Sec.\n",
      "112 / 112, Epoch:  80  Train Loss: 0.249348  Train Score: 0.933761  Valid Loss: 0.809575  Valid Score: 0.831250   34 Sec.\n",
      "112 / 112, Epoch:  81  Train Loss: 0.235337  Train Score: 0.938198  Valid Loss: 0.832702  Valid Score: 0.830990   34 Sec.\n",
      "Epoch    51: reducing learning rate of group 0 to 6.2500e-04.38114\n",
      "112 / 112, Epoch:  82  Train Loss: 0.239450  Train Score: 0.938114  Valid Loss: 0.837941  Valid Score: 0.837240   34 Sec.\n",
      "112 / 112, Epoch:  83  Train Loss: 0.246315  Train Score: 0.936077  Valid Loss: 0.834531  Valid Score: 0.840365   34 Sec.\n",
      "112 / 112, Epoch:  84  Train Loss: 0.236061  Train Score: 0.939872  Valid Loss: 0.842518  Valid Score: 0.831771   34 Sec.\n",
      "112 / 112, Epoch:  85  Train Loss: 0.226939  Train Score: 0.940318  Valid Loss: 0.856371  Valid Score: 0.831510   34 Sec.\n",
      "112 / 112, Epoch:  86  Train Loss: 0.232644  Train Score: 0.937891  Valid Loss: 0.819880  Valid Score: 0.833333   34 Sec.\n",
      "112 / 112, Epoch:  87  Train Loss: 0.232828  Train Score: 0.939900  Valid Loss: 0.814141  Valid Score: 0.840885   33 Sec.\n",
      "112 / 112, Epoch:  88  Train Loss: 0.222589  Train Score: 0.942132  Valid Loss: 0.835243  Valid Score: 0.837760   34 Sec.\n",
      "112 / 112, Epoch:  89  Train Loss: 0.236173  Train Score: 0.938030  Valid Loss: 0.827073  Valid Score: 0.839063   34 Sec.\n",
      "Epoch    59: reducing learning rate of group 0 to 3.1250e-04.42188\n",
      "112 / 112, Epoch:  90  Train Loss: 0.222303  Train Score: 0.942188  Valid Loss: 0.807346  Valid Score: 0.840104   34 Sec.\n",
      "112 / 112, Epoch:  91  Train Loss: 0.227307  Train Score: 0.940848  Valid Loss: 0.886018  Valid Score: 0.835938   34 Sec.\n",
      "112 / 112, Epoch:  92  Train Loss: 0.226527  Train Score: 0.941295  Valid Loss: 0.862864  Valid Score: 0.838281   34 Sec.\n",
      "112 / 112, Epoch:  93  Train Loss: 0.219561  Train Score: 0.943359  Valid Loss: 0.873937  Valid Score: 0.826563   34 Sec.\n",
      "112 / 112, Epoch:  94  Train Loss: 0.213313  Train Score: 0.944950  Valid Loss: 0.909166  Valid Score: 0.830208   34 Sec.\n",
      "112 / 112, Epoch:  95  Train Loss: 0.226702  Train Score: 0.942801  Valid Loss: 0.927913  Valid Score: 0.830729   34 Sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 / 112, Epoch:  96  Train Loss: 0.221309  Train Score: 0.944113  Valid Loss: 0.809337  Valid Score: 0.844792   34 Sec.\n",
      "112 / 112, Epoch:  97  Train Loss: 0.216712  Train Score: 0.945173  Valid Loss: 0.848414  Valid Score: 0.838802   34 Sec.\n",
      "Epoch    67: reducing learning rate of group 0 to 1.5625e-04.45759\n",
      "112 / 112, Epoch:  98  Train Loss: 0.216800  Train Score: 0.945759  Valid Loss: 0.903783  Valid Score: 0.826823   34 Sec.\n",
      "112 / 112, Epoch:  99  Train Loss: 0.215182  Train Score: 0.945368  Valid Loss: 0.848183  Valid Score: 0.836719   34 Sec.\n",
      "112 / 112, Epoch: 100  Train Loss: 0.217397  Train Score: 0.944503  Valid Loss: 0.881314  Valid Score: 0.831510   34 Sec.\n"
     ]
    }
   ],
   "source": [
    "train(epochs=70, start_epoch=31, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "def bce_lovasz(logits, labels):\n",
    "    loss1 = bce(logits, labels)\n",
    "    loss2 = lovasz_hinge(logits, labels)\n",
    "    loss = loss1 + loss2\n",
    "    return loss\n",
    "\n",
    "if CHECKPOINT:\n",
    "    criterion = lovasz_hinge\n",
    "    lr = 0.005\n",
    "else:\n",
    "    criterion = bce_lovasz\n",
    "    lr = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if CHECKPOINT is False:\n",
    "    train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lovasz_hinge\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7, verbose=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True, drop_last=True, collate_fn=tgs_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True, drop_last=True, collate_fn=tgs_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(epochs=70, start_epoch=31, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine(epochs, start_epoch=1, scheduler=None, early_stopping=False, best_checkpoint=True, verbose=1):\n",
    "    model.train()\n",
    "    iter_valid = 300\n",
    "    n_batch = len(train_loader)\n",
    "\n",
    "    best_validation_score = 0.0\n",
    "    best_train_score = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    log_fn = f'log/{VERSION}_{start_epoch}_{start_epoch+epochs-1}.log'\n",
    "    if os.path.exists(log_fn):\n",
    "        for i in range(1, 10):\n",
    "            log_fn = f'log/{VERSION}_{start_epoch}_{start_epoch+epochs-1}_{i}.log'\n",
    "            if not os.path.exists(log_fn):\n",
    "                break\n",
    "    log_file = open(log_fn, 'w')\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        epoch = start_epoch + e\n",
    "        t_ = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_score = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_score = 0.0\n",
    "        n_valid = 0\n",
    "        \n",
    "        for i, (img_batch, mask_batch) in enumerate(train_loader, 1):\n",
    "            img_batch = img_batch.float().cuda()\n",
    "            mask_batch = mask_batch.float().cuda()\n",
    "            outputs = net(img_batch) # nn.parallel.data_parallel(model, img_batch)\n",
    "            loss = criterion(outputs, mask_batch)\n",
    "            # loss2 = lovasz_hinge(outputs, mask_batch)\n",
    "            # loss = loss1 + loss2\n",
    "            score = iou(mask_batch, torch.sigmoid(outputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_score += score.item()\n",
    "            \n",
    "            if i % iter_valid == 0:\n",
    "                n_valid += 1\n",
    "                v_l, v_s = validation()\n",
    "                valid_loss += v_l\n",
    "                valid_score += v_s\n",
    "                print('%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f  Valid Loss: %.6f  Valid Score: %.6f\\r'\n",
    "                      % (i, n_batch, epoch, running_loss/i, running_score/i, valid_loss/n_valid, valid_score/n_valid), end= \"\")\n",
    "                continue\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print('%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f\\r'\n",
    "                      % (i, n_batch, epoch, running_loss/i, running_score/i), end= \"\")\n",
    "            scheduler.step()\n",
    "        # end for in enumerate(train_loader, 1)\n",
    "\n",
    "        n_valid += 1\n",
    "        v_l, v_s = validation()\n",
    "        valid_loss += v_l\n",
    "        valid_score += v_s\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step(valid_score/n_valid)\n",
    "        \n",
    "        # checkpoint utilities\n",
    "        if valid_score/n_valid > best_validation_score:\n",
    "            best_validation_score = valid_score / n_valid\n",
    "            best_train_score = running_score / i\n",
    "            best_epoch = epoch\n",
    "            if best_checkpoint:\n",
    "                model_state_dict = model.state_dict()\n",
    "                optimizer_state_dict = optimizer.state_dict()\n",
    "        if early_stopping:\n",
    "            if epoch - best_epoch == early_stopping:\n",
    "                break\n",
    "\n",
    "        log_message = '%3d / %d, Epoch: %3d  Train Loss: %.6f  Train Score: %.6f  Valid Loss: %.6f  Valid Score: %.6f  %3d Sec.'\\\n",
    "        % (i, n_batch, epoch, running_loss/i, running_score/i, valid_loss/n_valid, valid_score/n_valid, time.time() - t_)\n",
    "        print(log_message)\n",
    "        print(log_message, file=log_file)\n",
    "        epoch += 1\n",
    "\n",
    "    # end for in range(epochs)\n",
    "    \n",
    "    log_file.close()\n",
    "    # save model\n",
    "    if best_checkpoint:\n",
    "        f_ = f'{VERSION}_{best_epoch}_{best_validation_score:.5f}.pt'\n",
    "        torch.save({\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'best_validation_score': best_validation_score,\n",
    "            'best_train_score': best_train_score,\n",
    "        }, f_)\n",
    "\n",
    "    f_ = f'{VERSION}_{epoch}_{valid_score/n_valid:.5f}.pt'\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_score': running_score/n_batch,\n",
    "        'validation_score': valid_score,\n",
    "    }, f_)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "optimizer.param_groups[0]['lr'] = 0.01\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*epochs, 0.001)\n",
    "VERSION = 'tgs_1016_from_1006_cycle_1'\n",
    "train(epochs=epochs, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "optimizer.param_groups[0]['lr'] = 0.01\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*epochs, 0.001)\n",
    "VERSION = 'tgs_1016_from_1006_cycle_2'\n",
    "train(epochs=epochs, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    scheduler.step()\n",
    "    print(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*50, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center with maxpool on the last 100 epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(epochs=15, start_epoch=16)\n",
    "# 225 / 225, Epoch: 16  Train Loss: 0.607186  Train Score: 0.777750  Valid Loss: 0.628332  Valid Score: 0.768917   42 Sec.\n",
    "# 225 / 225, Epoch: 1 7  Train Loss: 0.593546  Train Score: 0.782167  Valid Loss: 0.593895  Valid Score: 0.775500   42 Sec.\n",
    "# 225 / 225, Epoch: 18  Train Loss: 0.597450  Train Score: 0.779139  Valid Loss: 0.658508  Valid Score: 0.756333   42 Sec.\n",
    "# 225 / 225, Epoch: 19  Train Loss: 0.588280  Train Score: 0.785333  Valid Loss: 0.624695  Valid Score: 0.767917   42 Sec.\n",
    "# 225 / 225, Epoch: 20  Train Loss: 0.568089  Train Score: 0.788000  Valid Loss: 0.612825  Valid Score: 0.770417   42 Sec.\n",
    "# 225 / 225, Epoch: 21  Train Loss: 0.570660  Train Score: 0.787667  Valid Loss: 0.626319  Valid Score: 0.767167   42 Sec.\n",
    "# 225 / 225, Epoch: 22  Train Loss: 0.553968  Train Score: 0.789778  Valid Loss: 0.609008  Valid Score: 0.775917   42 Sec.\n",
    "# 225 / 225, Epoch: 23  Train Loss: 0.556656  Train Score: 0.796278  Valid Loss: 0.605198  Valid Score: 0.768667   42 Sec.\n",
    "# 225 / 225, Epoch: 24  Train Loss: 0.535197  Train Score: 0.799500  Valid Loss: 0.603050  Valid Score: 0.768917   42 Sec.\n",
    "# 225 / 225, Epoch: 25  Train Loss: 0.519393  Train Score: 0.803361  Valid Loss: 0.579713  Valid Score: 0.784000   42 Sec.\n",
    "# 225 / 225, Epoch: 26  Train Loss: 0.547456  Train Score: 0.797694  Valid Loss: 0.616334  Valid Score: 0.767083   42 Sec.\n",
    "# 225 / 225, Epoch: 27  Train Loss: 0.553760  Train Score: 0.793083  Valid Loss: 0.617891  Valid Score: 0.772500   42 Sec.\n",
    "# 225 / 225, Epoch: 28  Train Loss: 0.507759  Train Score: 0.804611  Valid Loss: 0.602714  Valid Score: 0.784500   42 Sec.\n",
    "# 225 / 225, Epoch: 29  Train Loss: 0.532929  Train Score: 0.803806  Valid Loss: 0.606444  Valid Score: 0.782000   42 Sec.\n",
    "# 225 / 225, Epoch: 30  Train Loss: 0.537051  Train Score: 0.798556  Valid Loss: 0.582551  Valid Score: 0.782667   42 Sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lovasz_hinge\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epochs=70, start_epoch=101, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 225 / 225, Epoch: 16  Train Loss: 0.467769  Train Score: 0.737306  Valid Loss: 0.487338  Valid Score: 0.731250   45 Sec.\n",
    "# 225 / 225, Epoch: 17  Train Loss: 0.496532  Train Score: 0.722472  Valid Loss: 0.499559  Valid Score: 0.725083   42 Sec.\n",
    "# 225 / 225, Epoch: 18  Train Loss: 0.455637  Train Score: 0.744111  Valid Loss: 0.497060  Valid Score: 0.728917   42 Sec.\n",
    "# 225 / 225, Epoch: 19  Train Loss: 0.458069  Train Score: 0.735444  Valid Loss: 0.484206  Valid Score: 0.734250   42 Sec.\n",
    "# 225 / 225, Epoch: 20  Train Loss: 0.457995  Train Score: 0.736250  Valid Loss: 0.483421  Valid Score: 0.733333   42 Sec.\n",
    "# 225 / 225, Epoch: 21  Train Loss: 0.500013  Train Score: 0.720833  Valid Loss: 0.494749  Valid Score: 0.728167   42 Sec.\n",
    "# 127 / 225, Epoch: 22  Train Loss: 0.438417  Train Score: 0.745817  Valid Loss: 0.457279  Valid Score: 0.745500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(f_=None):\n",
    "    MODEL_PATH = './'\n",
    "    if f_ is not None:\n",
    "        checkpoint = torch.load(MODEL_PATH + f_)\n",
    "        return checkpoint\n",
    "    models = [f for f in os.listdir(MODEL_PATH) if 'pt' in f]\n",
    "    checkpoint = torch.load(MODEL_PATH + models[0])\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT:\n",
    "    checkpoint = load_checkpoint(f_='pytorch_develop_30_0.82125.pt')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    last_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT:\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=6)\n",
    "    train(epochs=15, start_epoch=last_epoch) # scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(TTA=True):\n",
    "    prediction = []\n",
    "    n_batch = len(test_loader)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, img_batch in enumerate(test_loader):\n",
    "            preds_batch = model(img_batch.cuda())\n",
    "            preds_batch_hf = model(img_batch.cuda().flip(3))\n",
    "            preds_batch = torch.sigmoid((preds_batch + preds_batch_hf.flip(3)) / 2)\n",
    "            prediction.append(preds_batch.cpu().detach().numpy())\n",
    "            print('%3d / %3d, prediction \\r' % (i+1, n_batch), end='')\n",
    "    model.train()\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def unpadding(preds):\n",
    "    return preds[:, :, 14: 128-13, 14: 128-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBMIT:\n",
    "    test_dataset = TGSdataset_test(images_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=30, num_workers=2, pin_memory=True, collate_fn=tgs_collate_test)\n",
    "    prediction = predict()\n",
    "    prediction = (prediction > best_threshold).astype('uint8')\n",
    "    prediction = list(map(rle_encoding, prediction))\n",
    "    prediction = [(' ').join(str(e)[1:-1].split(', ')) for e in prediction]\n",
    "    submission = pd.DataFrame()\n",
    "    submission['id'] = df_test.index\n",
    "    submission['rle_mask'] = prediction\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
